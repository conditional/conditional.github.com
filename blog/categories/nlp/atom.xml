<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: nlp | a lonely miner]]></title>
  <link href="http://conditional.github.io/blog/categories/nlp/atom.xml" rel="self"/>
  <link href="http://conditional.github.io/"/>
  <updated>2014-01-17T14:09:27+09:00</updated>
  <id>http://conditional.github.io/</id>
  <author>
    <name><![CDATA[Koji Matsuda]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[第5回 最先端NLP勉強会に参加してきました]]></title>
    <link href="http://conditional.github.io/blog/2013/09/08/report-of-snlp5/"/>
    <updated>2013-09-08T14:29:00+09:00</updated>
    <id>http://conditional.github.io/blog/2013/09/08/report-of-snlp5</id>
    <content type="html"><![CDATA[<p>もう一週間ほど前になってしまうのですが，<a href="http://www.logos.t.u-tokyo.ac.jp/snlp5/">最先端NLP勉強会</a> という会に参加させていただきました．</p>

<p>じつは昨年も参加するべく申し込みまでは行ったものの，事情があって参加できず．今年はなんとかリベンジを果たせました．</p>

<p>二日間で30本もの論文を読むこの勉強会，<strong>読む論文の選出プロセスにも工夫が凝らされて</strong>います．</p>

<ol>
<li>参加者全員が，対象となる会議の予稿集に目を通し，面白そうだと思った論文数本(今年は12本)に対して投票を行う．<!--例年は，ACL, NAACL, EMNLPが対象らしいのですが，今年はEMNLPの開催が遅いので，そのかわりに，新しく創刊されたTACL．--></li>
<li>多くの票を集めた論文，上位30本ほどを候補とし，参加者はその中から自分が紹介する論文を選ぶ．</li>
</ol>


<p>という二段階をとっているので，いわゆる「ハズレ」な論文が少なくなっており，どの発表もたいへん勉強になりました．</p>

<hr />

<p>私が紹介したのは以下の論文，</p>

<ul>
<li>Mohammad Taher Pilehvar, David Jurgens and Roberto Navigli, <a href="http://aclweb.org/anthology-new/P/P13/P13-1132.pdf"><em>Align, Disambiguate and Walk</em>  : A Uniﬁed Approach for Measuring Semantic Similarity</a>, ACL2013</li>
</ul>


<p><iframe src="http://www.slideshare.net/slideshow/embed_code/25994117" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen webkitallowfullscreen mozallowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/koji_matsuda/snlp5-matsuda" title="Align, Disambiguate and Walk : A Uniﬁed Approach forMeasuring Semantic Similarity" target="_blank">Align, Disambiguate and Walk : A Uniﬁed Approach forMeasuring Semantic Similarity</a> </strong> from <strong><a href="http://www.slideshare.net/koji_matsuda" target="_blank">Koji Matsuda</a></strong> </div>

</p>

<p>発表スライドはそのうち公開されるような気もしますが，今のところ認証が必要みたいなので，私のぶんだけ．
他の方の発表がすばらしいものばかりで，ちょっと手抜きすぎな感じがしたので，発表時のものから大幅に加筆しています．英語がおかしいのには，目をつぶってください・・・．</p>

<p><a href="/blog/2013/04/01/unsupervised-wsd-with-graph-connectivity/">以前紹介したGraph Connectivityに基づくWSDの論文</a>と同じ，Roberto Navigliのチームによる論文です．</p>

<p>WSDの論文というよりは，<strong>文間の意味的な類似度を精度良く求めるために，WSDを活用する</strong>，という主旨．いちおう，WSDのための新たなアルゴリズムも提案していますが，そちらは限定的な状況を想定しているため，直接一般のWSDに適用するのは難しいかも．</p>

<p>この論文の優れているところは，語義(WordNet上の一つのSynset)，単語，一つの文，(論文中では書かれていませんが，おそらくドキュメント全体も)といった，<strong>さまざまな粒度の言語的要素を，ひとつの表現</strong>(WordNet Synset上の確率分布)で表すことができる，というところ．</p>

<p>アイデアは単純で，文(とか単語とか，意味表現を求めたい言語要素)を語義の集合にバラして，それを種として，<strong>WordNetのグラフ上をランダムウォークさせる</strong>，というもの．
ただ，多義語が含まれる場合は種にノイズが混ざってしまうので，それをWSDで解決してから，グラフにつっこむという解決策を提案しています．</p>

<p>WSDのアルゴリズムは，<a href="http://en.wikipedia.org/wiki/Yarowsky_algorithm">Yarowskyのアルゴリズム</a>などと同様に"one sense per discourse"仮説に立脚したもので，二文それぞれが近い意味になるような語義の組み合わせを二文間のアラインメントで探索する，というもの．べつにこの方法でなければならない，ということもなさそうですが，もともとの目的が"文間の意味的な類似度"ですので，対になる文が意味的にだいたい似ている，ということが仮定できる状況下では，有効な方法に思えます．</p>

<p>ツッコミどころはいっぱいあって，例えば<strong>語順とか構文のような，文全体の意味を決めるのに重要な要素を捨象してしまっている</strong>とか，マルチシードのランダムウォークって，それシード一つのランダムウォーク単純に重ねあわせただけなのでは，とか，</p>

<p>勉強会での説明のときは，この論文が主に「文間の意味的な類似度」に最もフォーカスを当てていることがうまく説明できなかったのですが，帰りの電車でうんうん唸りながら考えて，ここに焦点をあてて説明すれば良かったな，と帰宅してからチマチマスライドなおすなど．</p>

<hr />

<p>その他，勉強会で紹介していただいたものの中で，いくつか気になった論文を．</p>

<ul>
<li>Mike Lewis and Mark Steedman, <a href="http://aclweb.org/anthology/Q/Q13/Q13-1015.pdf">Combined Distributional and Logical Semantics</a>, TACL Vol.1</li>
</ul>


<p>CCGパーシングに，distributional semanticsを結合する，という話．CCGのpredicateに，大きなコーパスをクラスタリングして得られたentity(項)とrelation(述語)を統合する，という研究．LogicとDistributional Semanticsは，それぞれ別個に発展してきているので，それらを一度まとめて考えてみよう，というモチベーションがありそう．組み合わせることで何が新しく解けるようになるのか，というのはまだオープンプロブレムみたい（そもそも，評価に使えるデータがない？）だけれど，今後が楽しみ．</p>

<ul>
<li>Dan Garrette and Jason Baldridge, <a href="http://aclweb.org/anthology/N/N13/N13-1014.pdf">Learning a Part-of-Speech Tagger from Two Hours of Annotation</a>, NAACL2013</li>
</ul>


<p>ルワンダ語とかマダガスカル語のような，電子的に扱える言語リソースが限られている語に対して，いかに小さな労力で品詞タガーを学習できるか，という問題を扱った研究．<a href="http://d.hatena.ne.jp/mamoruk/20130612">小町さんの日記</a>に大まかなストーリーが書いてあるのでそちらもご参照ください．個人的には，生のコーパスにアノテーションを付与するよりも，同じ時間であれば辞書のエントリ数を充実させたほうが若干性能が良い，つまり，語間のマルコフ性に関するデータよりも，語自体の品詞情報のほうが（人手のラベル付けにおいては），時間に対するコストパフォーマンスが良い，というのがなかなか興味深く感じました．</p>

<ul>
<li>Kai Zhao and Liang Huang, <a href="http://aclweb.org/anthology/N/N13/N13-1038.pdf">Minibatch and Parallelization for Online Large Margin Structured Learning</a>, NAACL 2013</li>
</ul>


<p>オンライン学習を並列化するためには，それぞれのノード間でパラメータを共有する必要があるのだけれど，そのタイミングをうまく調節することで，スケーラブルにしよう，という研究．とくに構造学習においては，デコード（新しいデータを評価してみて，パラメータの更新が必要かどうか判断する）というフェーズに時間がかかることが多いのだけれど，これはそれぞれのデータに対しては独立に行える（並列化できる）．しかしながら，間違えたたびにパラメータの更新を行っていてはオーバーヘッドが大きすぎるので，途中で間違えたとしても気にせず数事例まとめて評価する(ミニバッチ)ことで，パラメータの更新回数を少なくしよう，というアイディア．うーんうまく説明できないのですが，ミニバッチ，という考え方は Deep Learning でも重要とされていて，完全なオンライン(SGDとか)安定して収束しない，完全なバッチ（BFGSとか）だとメモリがキツい，みたいなときによく使われているので，おさえておいて損はなさそう．</p>

<ul>
<li>Carina Silberer, Vittorio Ferrari and Mirella Lapata, <a href="http://aclweb.org/anthology/P/P13/P13-1056.pdf">Models of Semantic Representation with Visual Attributes</a>, ACL 2013</li>
</ul>


<p>実世界の物体（クマとか，カップとか）と，テキスト上に現れる Bear, Cup などをうまく結びつけよう，という研究．SIFTとかHOGといった，CVの研究においてよく用いられている特徴量ではなく，「クマは茶色い」「クマは四本足だ」などといったような，より高レベルな特徴量(Visual Attributeと呼ばれています)を用いる方法を提案．この高レベルな特徴量は，人手で用意する必要があるようなのですが，画像の特徴と，テキストの特徴を結びつける，という話はなかなか興味深く感じました．</p>

<hr />

<p>全体としては， Grounding(実世界との対応付け？うまい日本語訳が分かりませぬ) がジャンルとして大きな注目を集めているという印象を受けました．マルチモーダル！</p>

<p>道具としては，CCGはもう何処にでも出てくる感じ．CFGなどの伝統的な文法に比べて，どの辺に利点があるのかイマイチ分かっていないので，よく勉強する必要がありそうです．</p>

<p>かなりムシムシする部屋に二日間缶詰になっていたので，汗かきな私にはちょっとシンドイ感もありましたが，みなさま質の高い発表ばかりで，とても楽しい時間を過ごすことができました．幹事のみなさま，発表者のみなさま，どうもありがとうございました．</p>

<p>あわせて読みたい：</p>

<ul>
<li>持橋先生の日記 : <a href="http://chasen.org/~daiti-m/diary/?201309a&amp;to=201309020#201309020">第5回最先端NLP勉強会</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ACL2013 マイ・リーディングリスト(1)]]></title>
    <link href="http://conditional.github.io/blog/2013/08/19/acl2013-reading-list-part-1/"/>
    <updated>2013-08-19T13:20:00+09:00</updated>
    <id>http://conditional.github.io/blog/2013/08/19/acl2013-reading-list-part-1</id>
    <content type="html"><![CDATA[<p>自然言語処理に関する最高峰の国際会議 <a href="http://www.acl2013.org/site/">ACL</a> が先日開催されました．
発表された論文をいくつか眺めてみたのでメモ．</p>

<p>ほんとうはもっといっぱい紹介したい論文があるのだけれど，時間の都合上，5本だけ．気がむいたら続編あるかも．</p>

<p>すべての論文は，<a href="aclweb.org/anthology/">ACL Anthology</a>で入手することができる．</p>

<hr />

<h3><a href="http://aclweb.org/anthology/P/P13/P13-1044.pdf">"Nonconvex Global Optimization for Latent-Variable Models"</a> Matthew R. Gormley and Jason Eisner, ACL 2013</h3>

<p>分枝限定法（評価関数の上限を推定しながら，探索木を刈りこむ）を用いて，隠れ変数モデルの大域解を求めよう，という話．
実験にはDMV(Dependency Model with Valence)というわりと有名な Unsupervised Dependency Parsingのモデルを用いている．
グラフを見る限り，劇的に性能が良くなっているわけではなさそう，というか，線がつぶれてよく分からないｗ</p>

<p>正直さっぱり分かっていないんだけど，分枝限定法についてのぼくの限られた知識から推測すると，
隠れ変数が離散値をとる場合しか使えないような気がする．
(正確にmarginalを計算しているのではなく，MAP解だけを数え上げているようにみえる)</p>

<p>実は，EMのような局所解に縛られる問題を，メタヒューリスティクスを使ってなんとかしよう，というのは，学部時代の恩氏が時々
言っていた話なのだけれど，当時は（いまも）能力が追いつかなくてぜんぜん歯が立ちませんでした．</p>

<p>すぐにどうこうする話ではなさそうだけれど，むかしから微妙に興味はあった話ではあるので，
（小さな局所解がいっぱいあるような）モデルを扱うときのためにいちおう記憶にとどめておきたい．</p>

<hr />

<h3><a href="http://aclweb.org/anthology/P/P13/P13-1034.pdf">"Scaling Semi-supervised Naive Bayes with Feature Marginals"</a> Michael R. Lucas and Doug Downey, ACL 2013</h3>

<p>Semi-supervised Naive Bayesというと，EMと組み合わせたNigamの有名な論文以来，きれいな構造とシンプルなアルゴリズムのため，
よく使われるのだけれど，データ全体を何度もなめないといけないため，計算量が問題だった．あと，EMを使う以上，モデルが変な方向へ飛んでいってしまう(局所解につかまる)ため，性能が安定しないという問題があって，ちょっと扱いづらい．</p>

<p>そこで，featureの各クラスにおける出現確率だけをunlabeledデータから求める，というアイディアに基づいて高速化をはかったのがこの論文．
ラベルつきデータが少数しか利用できない場合，featureの出現回数の"信頼性"が無くなってしまうので，大量に用意できるunlabeledデータを用いてパラメータを補正する，という感じ．</p>

<p>このとき，データが正例か負例かのどちらかに属する，という仮定を用いて，学習データにおける正例負例の割合をうまく取り込むことで精度を上げている．</p>

<p>Semi-supervised NBだと，わりと最近では<a href="http://www.icml-2011.org/papers/93_icmlpaper.pdf">Semi-supervised Frequency Estimate</a>(Su+ ICML'11)という，ワンパスで解いちゃう劇的に速い方法が提案されていて，
アイディアもとても似ている．精度では今回提案された手法が優れているよう．計算時間の比較はなかったけど，同じくらい？</p>

<p>Naive Bayesは超有名なので，バリエーションも超大量にあるのだけれど，NBの良いところの一つはマルチクラス分類が自然に行えるところだと思っているので，
データが2クラス限定になっちゃうこれはどうかなーと思ったりする部分もある．
もちろん，one-versus-allのような仕組みでいくらでもマルチクラスに拡張できるのですが．</p>

<p>このへんは，2010年代っぽくない，とか言う人もいるけど，最近でも時々掘って成果を出している人がいるみたい．</p>

<hr />

<h3><a href="http://aclweb.org/anthology/P/P13/P13-1021.pdf">"Unsupervised Transcription of Historical Documents"</a> Taylor Berg-Kirkpatrick, Greg Durrett and Dan Klein, ACL 2013</h3>

<p>このあいだ草津にいったときに，ちょっと趣きのある看板をみつけたので，</p>

<p><blockquote class="twitter-tweet"><p>そういや白根山でみつけたこの看板、自然言語処理に対する挑戦っぽい <a href="http://t.co/ps3qgUCGfO">pic.twitter.com/ps3qgUCGfO</a></p>&mdash; Koji Matsuda (@conditional) <a href="https://twitter.com/conditional/statuses/349186418167406593">June 24, 2013</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<p>とか言ってたら，マジで論文が出てきた・・・．主に，むかしのタイプライターとか活版印刷で印刷された，
現在では読みにくくなってしまっている文書をいかにOCRするか，という題材．</p>

<p>モデルは， 言語モデル × typesettingモデル(文字のレイアウト：右にこれくらいズレてる，とか) × inkingモデル(インクのつけすぎで太くなっちゃってるとか) × ノイズモデル(経時変化による文字の欠けとか)を一つひとつ丁寧に組み上げて，Jointしている．
学習はEM．ちゃんと動くのか心配だけど，一つ一つの変数がちゃんと意味を持つよう工夫しているからか(むやみに変数が多くなりすぎないように工夫しているようにみえる)，
ちゃんと学習できているらしい．</p>

<p>学習したモデルでは，人工的にインクをダバ〜っとこぼした原稿も解読できたりするようで，このへんもなかなか興味深い．モデル自体の解釈も面白そう．</p>

<hr />

<h3><a href="http://aclweb.org/anthology/P/P13/P13-1036.pdf">"Scalable Decipherment for Machine Translation via Hash Sampling"</a> Sujith Ravi, ACL 2013</h3>

<p>暗号解読のノリで，言語モデル+サンプリングを使って対訳コーパスなしで翻訳をしちゃうお話の続編．
2011年の初出以来，年々進化を続けていて，今回は主に高速化がテーマになっている．</p>

<p>最初はword-to-wordの置換に基づく翻訳だけだったのが，これまでの研究で，さまざまなfeatureが使えるようになったのだけれど，
ちょっと計算に時間がかかりすぎるようになってきたので，Hash Samplingという手法で高速化を行っている．</p>

<p>Hash Samplingというのは，あんまりよくわかっていないのだけれど，ベクトル同士の類似度をはかる
(指数分布族からのサンプリングに用いる尤度項のテイラー近似は，パラメータベクトルと事例ベクトルの内積を用いて表せるらしい)
ときに，feature vectorの空間で直接内積をとる代わりに，適当なハッシュ関数を通して，その間の差分(ハミング距離とか)を用いる方法のよう．</p>

<p>この手法によって，現在のパラメーターに近い語は高い確率でサンプルされ，そうでない語は低い確率，というサンプリングを高速に実現している．
{0,1}ベクトルのハミング距離は最近のCPUだとかなり高速に計算できるようで，BLEU値をほとんど落とさずに，2桁くらいの高速化を実現したらしい．</p>

<p>Hash Samplingは，かなり汎用性が高い高速化手法っぽいので，そのうちじっくり読む機会があればいいなと思っている．
テイラー展開による近似 + ハッシュによる近似，という2段階の近似を経ているあたりが少し気になったりはする．</p>

<hr />

<h3><a href="http://aclweb.org/anthology/P/P13/P13-1127.pdf">"From Natural Language Speciﬁcations to Program Input Parsers"</a> Tao Lei, Fan Long, Regina Barzilay, and Martin Rinard, ACL 2013</h3>

<p>ICPCなどのプロコンの問題を，コンピュータに解かせよう！というモチベーションがありそうな研究．
といっても，今回はまだ問題を解くところまでは行っていなくて，入力データの解析部分に対する挑戦．
プロコンの問題では，たいてい，</p>

<blockquote><p>入力は複数のデータセットからなる． 各データセットは2つの整数 a0  と L  が1個のスペースで区切られた1行であり，
a0  が最初の整数を， L  が桁数を表す． ただし， 1 ≤ L  ≤ 6 かつ 0 ≤ a0  &lt; 10L である．</p>

<p>from <a href="http://judge.u-aizu.ac.jp/onlinejudge/description.jsp?id=1180&amp;lang=jp">AIZU ONLINE JUDGE</a></p></blockquote>

<p>というような入力データのフォーマット指示がなされているので，これを構文解析して，パーサーを構成するC++プログラムを自動で生成しよう，というテーマ．</p>

<p>用いることの出来るデータは，上に挙げたようなフォーマット指示の問題文(英語)と，実際にプロコン主催者から与えられる入力データ．
直接プログラムそのものへの変換を考えるのではなく，フォーマット指示文から，データ構造を表す "specification tree"を生成し，
それを通してparsingプログラム(より正確には，yaccやbisonに入力するような形式文法)の生成を行っている．</p>

<p>モデルは，
テキスト中のそれぞれの要素が，他の要素とどのような関係を持っているかを当てるための dependency parsing と，
 それぞれのノードがプログラム内でどのような役割をもつか当てるための role labeling のふたつの要素で構成されている．</p>

<p>学習は，だいたい次のような流れ．</p>

<ol>
<li>現在のモデルから specification tree をサンプリングする</li>
<li>サンプリングされたtreeからプログラムを生成し，テスト用データをすべてパースできるか調べる(feedbackと呼ばれている)</li>
<li>treeがどれくらい良いか，に応じて，Metropolis-Hasting ruleで次の状態を定める</li>
</ol>


<p>このfeedbackと呼ばれる学習手法によって，F値が54から80程度に向上したらしい．
<a href="http://people.csail.mit.edu/taolei/papers/acl2013-slides.pdf">著者による発表スライド</a>もあるので，興味をもって頂けましたら，ぜひそちらも．</p>

<p>つづく・・・かも？</p>
]]></content>
  </entry>
  
</feed>
