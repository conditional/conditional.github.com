<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: learning | a lonely miner]]></title>
  <link href="http://conditional.github.io/blog/categories/learning/atom.xml" rel="self"/>
  <link href="http://conditional.github.io/"/>
  <updated>2014-01-31T14:07:45+09:00</updated>
  <id>http://conditional.github.io/</id>
  <author>
    <name><![CDATA[Koji Matsuda]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[[MLAC 2013 7日目] Torch7でお手軽ニューラルネットワーク]]></title>
    <link href="http://conditional.github.io/blog/2013/12/07/an-introduction-to-torch7/"/>
    <updated>2013-12-07T21:29:00+09:00</updated>
    <id>http://conditional.github.io/blog/2013/12/07/an-introduction-to-torch7</id>
    <content type="html"><![CDATA[<h2>はじめに</h2>

<p>この記事は <a href="http://qiita.com/advent-calendar/2013/machinelearning">Machine Learning Advent Calendar 2013</a> の 7日目の記事です．</p>

<p>2013年，<em>Deep Learning</em> もアカデミックレベルではさまざまな分野への浸透が進み，バズワードの域を脱したように思えます．
これまでは，機械学習というと，応用分野においては(分類/回帰といった)タスクを決めてしまった上でブラックボックスとして
扱うもの，という空気がありましたが，
<em>Deep Learning</em> に代表される柔軟な，いかようにも組み上げられるモデルは，問題の性質を積極的に(特徴量としてではなく，モデル自体に)組み込むことを容易にする，大きな武器になるのではないかと感じています．</p>

<p>素性エンジニアリング vs モデルパラメータエンジニアリング の不毛な戦いが幕を上げた，という見方もできちゃいそうですが・・・．．</p>

<p>さて今回は， <a href="http://torch.ch/">Torch7</a> という，Neural Networkを中心とした機械学習向けの環境をごくごく簡単に紹介します．
Torch7自体は，比較的最近公開されたソフトウェアですが，”7"という文字から伺えるとおり，
主開発者の Ronan Collobert 氏が中心となって，かれこれ10年以上継続的に開発されているパッケージのようです．</p>

<p>論文としては， <a href="http://data.neuflow.org/pubs/biglearn11.pdf">NIPS’11 BigLearn Workshop のもの</a>が初出でしょうか．
Neural Network向けのソフトウェアパッケージというと，
<a href="http://deeplearning.net/software/theano/">Theano</a>/<a href="http://deeplearning.net/software/pylearn2/">Pylearn2</a> が主流ですが(要出典)，Torch7の特徴をいくつか挙げてみましょう．</p>

<ul>
<li>スクリプティング言語として， Lua が採用されています．
　そのため，他のアプリケーションにおけるスクリプティング環境としての組み込みが容易になっているそうです．
　試していないのですが，iOSアプリへの組み込みも可能だとか・・・？</li>
<li>テンソルのサポートが手厚く，次元を変えたりメモリ上の配置を変えたり，四則演算を行ったり，がらくちん</li>
<li>NNを構成するモジュールが大量に備わっており，ブロックを組み立てる感覚でモデルを作ることができる</li>
<li>独自のオブジェクトシステムが備わっており，OOPっぽいプログラムが可能</li>
<li>BLAS, CUDAなどのサポート</li>
<li><strong>なぜかIDEがついてくる！！</strong> (<code>torch -ide</code>で起動できます)</li>
</ul>


<p>Theano/Pylearn2との比較については，Theano開発チームによる以下の資料(とくに Table 1)が参考になります．</p>

<ul>
<li><a href="http://arxiv.org/pdf/1211.5590v1.pdf">Theano: new features and speed improvements</a></li>
</ul>


<p><img class="center" src="/images/torch/features.png" width="650"></p>

<p>Theano は Deep Learning の総本山であるモントリオール大のチームが強力に推進しているプロジェクトだけあって，
機能面での対抗はなかなか厳しそうですが，頑張ってます，，，頑張っています！</p>

<h2>Torch7のセットアップ</h2>

<p><a href="http://torch.ch/">Torch7</a> よりどうぞ．</p>

<p>インストールしたのがしばらく前なので記憶が曖昧なのですが，私の環境(MacOS X 10.9)では，特に引っかかるところはなかったように思います．要<code>cmake</code>．</p>

<p>私は使いませんでしたが，<code>apt</code>が動くLinuxか，<code>homebrew</code>が動くMacなら，インストールスクリプトも用意されています．
しかし，こちらのインストールスクリプト，若干お行儀が悪い気もするので，気になる方は実行前に一度眺めてみてください．</p>

<p>各種の拡張機能(カメラへのアクセスなど)は，<code>luarocks</code>というパッケージマネージャ(Rubyにおける<code>gem</code>,Pythonにおける<code>pip</code>のようなもの)を用いて管理するようです．</p>

<h2>Torch7で多層パーセプトロン</h2>

<h3>ネットワークの構築</h3>

<p>Torch7 における Neural Network の構築は非常に簡単です．
たとえば，基本的な中間層1層のネットワークは，以下のようなコードで表現できます．</p>

<p><code>lua
 require “nn"
 mlp = nn.Sequential()        -- Multi Layer Perceptron
 mlp:add( nn.Linear(1000, 25) ) -- 1000 input, 25 hidden units
 mlp:add( nn.Tanh() ) -- hyperbolic tangent transfer function
 mlp:add( nn.Linear(25, 10) ) -- 10 output
 mlp:add( nn.SoftMax() ) -- softmax output
</code></p>

<p>このネットワークを，トレーニングデータに対する負の対数尤度を目的関数として訓練するには，</p>

<p><code>lua
 criterion = nn.ClassNLLCriterion()
 trainer   = nn.StochasticGradient(mlp, criterion)
 trainer:train(dataset)
</code></p>

<p>とするだけです．明解ですね．</p>

<p><code>“nn"</code>パッケージには，約80種類のさまざまな Module (NNの層や目的関数に相当)が用意されており，これらを自由に組み合わせてネットワークを作ることが可能です．
行う価値や効率的な最適化法があるかどうかは別にして， <code>add()</code> を用いてどんどん繋げていけばいくらでも <em>Deeeeeeeeeep</em> にすることも簡単にできそうです．</p>

<h3>学習過程の制御</h3>

<p>基本的には以上なのですが，最近の Neural Network の訓練で必須になってきている mini-batch や momentum のような技術を用いたり，
準ニュートン法や共役勾配法などのより高度な最適化を行うために，より細かく学習の制御を行う方法も用意されています．</p>

<p>``` lua
 parameters,_ = mlp:getParameters()
 feval = function(x)</p>

<pre><code>       -- input : 入力事例テンソル , target : inputに対する正解ラベル(一般にはテンソル)
       local output = mlp:forward(input)     -- 前向きに伝播させて，出力を得る
       local f      = criterion:forward(output, target) --　決められた Criterion の元での目的関数の値を計算
       local df     = criterion:backward(output, target) -- 勾配を計算
       return f, df   -- 目的関数の値と，その勾配を返す
     end
</code></pre>

<p> optim.lbfgs(feval, parameters) -- parametersを更新
 --（注意）細かい部分を省いた擬似コードなので，このままでは動かないと思います XD
```</p>

<p>というように，現在のパラメータの元での目的関数の値と，その勾配を返すようなクロージャを作ることで，
 <code>“optim”</code> パッケージで用意されているさまざまな最適化アルゴリズム(SGD, AdaGrad, L-BFGSなど・・・)を用いることも可能です．</p>

<p>本稿では触れませんが，<code>module:forward()</code>, <code>criterion:backward()</code> でネットワークの出力，勾配を得て <code>module:gradUpdate()</code> で直接更新するという手段も用意されています．</p>

<h2>ためしてみる(数字認識タスク)</h2>

<p>ここでは，　<a href="http://code.cogbits.com/wiki/doku.php?id=start">Machine Learning with Torch7</a> で用いられているチュートリアルコードを実際に動かしてみます．</p>

<p><code>
 git clone https://github.com/clementfarabet/torch-tutorials.git
</code></p>

<p>でお手元にクローンして， <code>2_supervised</code> の下にもぐってみてください．</p>

<p><code>
 torch ./doall.lua
</code></p>

<p>でデータセットのダウンロードを含めた実験スクリプト全体の実行が始まります．初回は 300MB くらいのファイルをいきなり <code>wget</code> しはじめるのでご注意ください．</p>

<p>ここで扱っているのは， <a href="http://ufldl.stanford.edu/housenumbers/">Street View House Numbers (SVHN)Dataset</a>という，
カラー画像から数字を認識するタスク，Format 2(32x32の画像)のほうです．</p>

<p><img class="center" src="/images/torch/SVHN32x32eg.png" width="480"></p>

<p><code>
 torch ./doall.lua -size small -model linear -batchSize 100 -plot
</code></p>

<p>とかすると，ごく普通の線形モデルでの最適化が始まり，その過程が <code>gnuplot</code> でプロットされていきます．</p>

<p><code>2_model.lua</code> には線形モデル(<code>linear</code>),隠れ層1層のニューラルネット(<code>mlp</code>),<a href="http://arxiv.org/abs/1204.3968">2-stageのたたみこみニューラルネット</a>(<code>convnet</code>)
が定義されており， <code>-model</code> オプションで切り替えることが可能です．
おフロに入っている間，40分くらい回した結果は以下のような感じです．</p>

<ul>
<li><code>mlp</code>モデル</li>
</ul>


<p><img class="center" src="/images/torch/test_mlp_15epoch.png" width="480"></p>

<ul>
<li><code>convnet</code>モデル</li>
</ul>


<p><img class="center" src="/images/torch/test_convnet_13epoch.png" width="480"></p>

<p>うーん，<code>mlp</code>はまだまだ伸びそうだなぁ．<code>convnet</code>は局所解につかまってしまったのでしょうか．時間切れにつき，あまり直感に沿った結果は出せませんでしたが，学習によって正解率が向上している，ということは一応みてとれます．</p>

<h2>Torch7 をより深く知るために</h2>

<p>最後に，いくつか資料へのリンクをまとめておきます．</p>

<ul>
<li><a href="http://ronan.collobert.com/pub/matos/2012_implementingnn_springer.pdf">Implementing Neural Networks Eciently</a></li>
</ul>


<p>メイン開発者の Ronan Collobert 氏による Torch7 の解説です．ソフトウェア全体のアーキテクチャや， Lua を採用した理由，実際のパフォーマンスなどがまとめられています，</p>

<ul>
<li><a href="http://code.cogbits.com/wiki/doku.php?id=start">Machine Learning with Torch7</a></li>
</ul>


<p>上に挙げたチュートリアル資料です．Auto-Encoder のサンプルや，新しいモジュールのプログラミング方法など，本稿では触れられなかったトピックが盛りだくさんです．これを遊んでみるだけで年を越せるかも・・・</p>

<ul>
<li><a href="https://groups.google.com/forum/#!forum/torch7">torch7 - Google グループ</a></li>
</ul>


<p>流量はさほど多くありませんが，盛んに開発中のソフトウェアですので，こちらを追うことも重要かと思います．</p>

<ul>
<li><a href="http://www.slideshare.net/yurieoka37/ss-28152060">実装ディープラーニング - slideshare</a></li>
</ul>


<p>@0kayu さんによる，Pylearn2を中心としたディープラーニングの実装に関するよいまとめスライドです．</p>

<ul>
<li><a href="http://kiyukuta.github.io/2013/09/28/casualdeeplearning4nlp.html">自然言語処理まわりのDeep Learningを自分なりにまとめてみた — KiyuHub</a></li>
</ul>


<p>そもそもなんで自然言語処理においてニューラルネットが流行ってるの？というあたりに焦点を当てた資料です．ガッツリ！</p>

<h2>まとめ</h2>

<p>Lua を用いて Neural Network を簡単に構築できる， Torch7 というソフトウェアを紹介いたしました．</p>

<p>Theano ひとり勝ちというのはいかにもつまらないので，皆様にお手にとって頂くきっかけになれば幸いです．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deep Learning : Bengio先生のおすすめレシピ]]></title>
    <link href="http://conditional.github.io/blog/2013/09/22/practical-recommendations-for-gradient-based-training-of-deep-architectures/"/>
    <updated>2013-09-22T10:09:00+09:00</updated>
    <id>http://conditional.github.io/blog/2013/09/22/practical-recommendations-for-gradient-based-training-of-deep-architectures</id>
    <content type="html"><![CDATA[<p>先日，身内の勉強会(&amp;ラボの勉強会)で，Deep Learningについてお話してきました．これまで興味がなさそうだったのに何故急に？というのはおいておいて．</p>

<p>紹介したのは，Deep Learningの第一人者のひとり， <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Yoshua Bengio先生</a>自身が執筆された，以下の論文．</p>

<ul>
<li>Yoshua Bengio, <a href="http://arxiv.org/abs/1206.5533">Practical recommendations for gradient-based training of deep architectures</a>, arXiv:1206.5533v2, 2012</li>
</ul>


<p>どうやら書籍の草稿のようで，<strong>Bengio先生の長年の研究で得られたさまざまなノウハウ</strong>(最近の手法まで)がぎっしり詰め込まれています．すごい．</p>

<p>以前から気にはなりつつも，ちょっと分量が多い(30ページくらいある)ので，なかなか手を出すことができなかったのですが，ようやくヤル気が出てきたので，ちょっとがんばって読み込んでみました．</p>

<p>スライドも置いておきます．50枚くらいあります（作るのしんどかった・・・．</p>

<p><iframe src="http://www.slideshare.net/slideshow/embed_code/26420875" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen webkitallowfullscreen mozallowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/koji_matsuda/practical-recommendation-fordeeplearning" title="ractical recommendations for gradient-based training of deep architectures" target="_blank">ractical recommendations for gradient-based training of deep architectures</a> </strong> from <strong><a href="http://www.slideshare.net/koji_matsuda" target="_blank">Koji Matsuda</a></strong> </div>

</p>

<p>Deep Learningは，その性能の高さから各所でブームになっていますが，実際に手を出してみると<strong>ハイパーパラメータがやたらと多く，最適化も難しい</strong>，ということで，使いこなしにくい技術になってしまっている感があります．</p>

<p>この記事では，Bengio先生が長年蓄積されたきたノウハウをもとに，</p>

<ul>
<li>どんなハイパーパラメータがあるか</li>
<li>どのように最適化すればよいか</li>
<li>うまく動かないときは，何をチェックすればよいか</li>
<li>学習の結果得られたネットワークを解釈するための可視化の方法</li>
</ul>


<p>など，実問題に適用する場合に役立つ，様々なアドバイスがまとめられています．</p>

<p>特に興味深かったのは，<strong>ハイパーパラメータはグリッドサーチするのではなく，ランダムサンプリングしたほうが性能が出る場合が多い</strong>よ，という知見．</p>

<p>何故なのか，というのは，スライドを見ていただいたり，以下の文献</p>

<ul>
<li>James Bergstra, Yoshua Bengio, <a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf">Random Search for Hyper-Parameter Optimization</a>, Journal of Machine Learning Research, 13(Feb):281−305, 2012.</li>
</ul>


<p>を読んで頂ければ合点がいくのではないかと思います．</p>

<p>これが機械学習一般に対して言えるか，というと必ずしもそうではなく，SVMのようにハイパーパラメータが少ないモデルでは成り立たないのではないかとは思いますがが，なかなか衝撃的な事実です．は〜グリッドサーチとは一体なんだったのか．</p>

<p>この論文を読めばDeep Learningが自在に使えるようになる，ということは無さそうですが，上手くいかず悩んでいる方がいらっしゃいましたら，少しでも参考にしていただければ幸いです．</p>

<hr />

<p>ところで，ふだんはPowerPointを使ってスライドを書いているのですが，今回は<a href="https://bitbucket.org/rivanvx/beamer/wiki/Home">Beamer</a>を使って作ってみました．まじめに使ったのは初めて．</p>

<p>PowerPointと比較すると，以下のような感想を持ちました．</p>

<ul>
<li>あらかじめ用意されたスタイルから逸脱したことは非常にやりにくくなっているので，よくある「文字が小さすぎてギッシリ」みたいな読みにくいスライドは作りにくく，結果として読みやすいものができあがる</li>
<li>タイプセット&amp;PDF生成に時間がかかるので(今回の場合，最終的に20秒くらいかかるようになってしまいました)，枚数がふえるとけっこうイライラ(要素の位置をちょっと調整する，とかで20秒かかるのはアホらしい)</li>
<li>図を入れるのがめんどい．PPTならコピペでよいところが，キャプチャしてバウンディングボックス作ってincludegraphics…</li>
<li>要素の入れ子の嵐になる(PPTならタブ一個でいいところが，itemize環境の追加になる)ので，可読性に劣る

<ul>
<li>Markdownとかから変換できればいいな〜</li>
</ul>
</li>
<li>TeXの(バッド)ノウハウ・・・</li>
</ul>


<p>全体的に，準備に時間がかけられて，かつソコソコ長いプレゼン(20枚〜)であれば，使う価値はあると思います(ただしTeX慣れしてるのが大前提)</p>

<p>ただ，長すぎるとタイプセットの時間でイライラするようになるので，一長一短ですね．</p>

<p><a href="ftp://ftp.u-aizu.ac.jp/pub/tex/CTAN/macros/latex/contrib/beamer/doc/beameruserguide.pdf">公式のリファレンスマニュアル</a>は異様に詳しいのですが，どう見ても入門向きではないですし，読む気も起きないと思いますので，まずは<a href="http://www.uncg.edu/cmp/reu/presentations/Charles%20Batts%20-%20Beamer%20Tutorial.pdf">A Beamer Tutorial in Beamer</a>あたりを参考にされるとよいのではないかと思います．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[コンピュータが政治をする時代(あるいは，行列とテキストの結合モデル)について]]></title>
    <link href="http://conditional.github.io/blog/2013/08/03/joint-modeling-of-a-matrix-with-associated-text-via-latent-binary-features/"/>
    <updated>2013-08-03T14:12:00+09:00</updated>
    <id>http://conditional.github.io/blog/2013/08/03/joint-modeling-of-a-matrix-with-associated-text-via-latent-binary-features</id>
    <content type="html"><![CDATA[<p>前回のVanishing Component Analysisに関する記事が思いのほか好評だったようで，
なんか自分に対してのハードルあげちゃった感あったり，記事冒頭でデブとか書くんじゃなかった・・・（ハハハ）と後悔してたり．．．</p>

<p>いつもどおり肩肘はらずに書きますね．例によって，マンスリー読み会で紹介した論文について．</p>

<ul>
<li><a href="http://books.nips.cc/papers/files/nips25/NIPS2012_0733.pdf">"Joint Modeling of a Matrix with Associated Text via Latent Binary Features"</a> XianXing Zhang and Lawrence Carin
, NIPS 2012.</li>
</ul>


<p><iframe src="http://www.slideshare.net/slideshow/embed_code/24888804" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen webkitallowfullscreen mozallowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="http://www.slideshare.net/koji_matsuda/joint-modeling-ofamatrixwithassociatedtext" title="Joint Modeling of a Matrix with Associated Text via Latent Binary Features" target="_blank">Joint Modeling of a Matrix with Associated Text via Latent Binary Features</a> </strong> from <strong><a href="http://www.slideshare.net/koji_matsuda" target="_blank">Koji Matsuda</a></strong> </div>

</p>

<p>行列と，その列or行に紐づいたテキストが存在するときに，行列をいかによくモデル化するか，というお話です．推薦システム界隈ではわりとよくある感じの話ではあると思うのですが，適用しているデータが面白かったので紹介．</p>

<p><img class="center" src="/images/joint_modeling_matrix_and_text.png" width="650"></p>

<p>なんと，<strong>アメリカ下院議員の(法案に対する)投票行動</strong>をモデル化しています．この問題ならではの面白い性質を使っているのかと思いながら読み進めたら実はそんなことも無くて肩透かしを食らった感じではあるのですが．</p>

<p>法案が議会を通過するか（採択されるか）を予測する，というタスクでは，<strong>法案のテキストだけ</strong>が与えられた状況で，<strong>90%以上の正解率</strong>を達成しています．
この数値が高いのか低いのか私には判断がつきかねる（そもそも，提出された法案がほとんど採択されるような状況では予測の意味が無い）ものの，かなり工夫の施された他手法に比べてもそれなりに高い性能を達成しているようです．
特に政治ドメインならではのモデリングを行っているわけではなく，かなりGeneralなモデルなのですが，たとえば，政治に特化した工夫がなされたIdeal Point Topic Model(IPTM)のような既存手法に比べても高い性能，というのはなかなか面白いところです．</p>

<p>割とよくある推薦システムの技術が，<strong>議会という，民主主義の根幹を担うおそらく極めて高度なシステムを，表面的にではあるもののエミュレーションできてしまう</strong>，というのはなかなか考えさせられるものがあります．</p>

<p>今回の論文では，「議員の過去の投票行動と，法案のテキスト」の間の関係をモデル化しているだけなのですが，外部の情報，たとえば経済指標だとかパブリックコメントだとか，を組み込むことができるとすると（能力のある研究者がその気になれば不可能ではないとはおもいます），<strong>コンピュータが政治を行う，という時代も意外と近づいてきているのかも</strong>しれません．</p>

<p>は〜ディストピアっぽい．</p>

<hr />

<p>以下，テクニカルな部分について．私は推薦システムだとかベイズモデルだとかはあんまり詳しくないので，色々間違っているかも．</p>

<p>この論文では，テキストを<strong>Focused Topic Model</strong>というトピックモデル，賛否を表す行列を<strong>Binary Matrix Factorization</strong>という行列分解モデルで表現しています．</p>

<p>そのうえで，<strong>全体を繋ぎあわせる</strong>ようなバイナリの変数を導入して，<strong>全体を一気にMCMCで推定</strong>する，という流れ．スライドの最後にも載せておきましたが，グラフィカルモデルにいろいろアノテーションしたものを貼っておきます．小さくて見づらい場合は「名前をつけて保存」とか，よしなに扱って頂いてかまいません．青四角はハイパーパラメータです．</p>

<p><img class="center" src="/images/joint_modeling_matrix_and_text_model.png" width="650"></p>

<p>そもそも，何故Focused Topic Modelなのか，何故Binary Matrix Fctorizationなのか，という部分には，論文中にはあまり説明がなかったのですが，ちょっと考えてみたらいくつか理由付け，というか動機の推察ができたのでメモしておきます．</p>

<p>一つは，Focused Topic Modelの性質として，IBPから生成されるバイナリベクトルをトピック分布のサブセット選択に用いているため，トピック分布をスパースになる，というものがあります．スパースであること自体はさほど重要ではないのですが，<strong>IBPから出てくるバイナリのベクトルが，Binary Matrix Factorizationのバイナリ表現とうまくマッチする</strong>，というのは考えられそうです．両者が等価なものだとはあまり思えないのですが，<strong>複数のモデルの間にインタラクションを持たせる</strong>ための確率変数としては，なかなかよい選択に思えます．</p>

<p>あとは，Focused Topic Modelの論文でも述べられている，<strong>コーパスレベルでのトピック生起確率と，個々のドキュメントでのトピック生起確率の相関</strong>に関する問題が，"法案"というドメインにおいては顕著に現れるという可能性もあります．私はアメリカの法律なんて読んだことないのですが，もし一つ一つの法案が(互いに語彙の重なりが少ないという意味で)専門的な文書になっているとすれば，Focused Topic Modelの利点が効いてくるのかもしれません．</p>

<p>しかし，これくらいごちゃごちゃしたグラフィカルモデルをまじめに読み解くのは初めてで，なかなか骨が折れました．低ランク近似の周辺なんかは未だにイマイチつかめていませんし，Inferenceのところは理解を諦めてほぼ眺めただけですが，それでもたいへん勉強になりました．</p>

<p>さて，読んでばっかいないで実装しろ，という声が聞こえたので今日はこのへんにて．とりあえず，ビールでも飲みに行ってきます．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[どんなデータでも(※)線形分離可能にしてしまう技術，Vanishing Component Analysis(ICML 2013)を紹介してきました]]></title>
    <link href="http://conditional.github.io/blog/2013/07/10/vanishing-component-analysis/"/>
    <updated>2013-07-10T11:30:00+09:00</updated>
    <id>http://conditional.github.io/blog/2013/07/10/vanishing-component-analysis</id>
    <content type="html"><![CDATA[<p>急に蒸し暑くなってきましたね．でぶちんなのでけっこうこたえます．タイトルはちょっと釣り気味．ビビっと来た方は是非論文に目を通してみてください:)</p>

<p>例によって，仲間内でやっている小さな勉強会で論文紹介をしてきましたので，そのご紹介です．ぼくの専門というか興味の中心は自然言語処理なので，ふだんはそっち方面を追っているのですが，勉強会では機械学習方面を中心にいろいろ読んでみてます．</p>

<p>今回は岡野原さんのこのツイートで興味を持った以下の論文を読ませていただきました．名前もかっこいい．<strong>ヴァニッシングコンポーネントアナリシス！</strong></p>

<p><blockquote class="twitter-tweet"><p>ICML2013のbestpaper。データ中の集合（例えば画像中の8の字など）が0になるような生成多項式を求める（=集合のコンパクトな表現）効率的なアルゴリズムを提案し教師有学習時の特徴生成などに使える。すごい <a href="http://t.co/DedSoyLaJR">http://t.co/DedSoyLaJR</a></p>&mdash; 岡野原 大輔 (@hillbig) <a href="https://twitter.com/hillbig/statuses/347504853205000193">June 20, 2013</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<ul>
<li><a href="http://jmlr.org/proceedings/papers/v28/livni13.pdf">"Vanishing Component Analysis"</a> Roi Livni et al, ICML 2013 (Best Paper)</li>
</ul>


<p><iframe src="http://www.slideshare.net/slideshow/embed_code/24079705" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen webkitallowfullscreen mozallowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="http://www.slideshare.net/koji_matsuda/vanishing-component-analysis" title="Vanishing Component Analysis" target="_blank">Vanishing Component Analysis</a> </strong> from <strong><a href="http://www.slideshare.net/koji_matsuda" target="_blank">koji_matsuda</a></strong> </div></p>

<p>タイミングよく，<a href="http://partake.in/events/0ae21389-aa2a-42c1-a247-f93582127216">ICML2013読み会</a>という企画に乗っかるチャンスを頂けたので，今回はそちらでもお話させていただきました．会場には40人ほどいらっしゃったでしょうか．思いのほか多くの方が集まっており，演台から会場をみたときにちょっと圧倒されちゃいました・・・．外部でお話するのはとても久しぶりだったのですが，たいへん楽しい会でした．主催の <a href="http://twitter.com/sla">@sla</a> さん，会場を提供してくださった中川先生，ありがとうございました．</p>

<p>この論文は，PCAやICAに代表される<strong>Component Analysis</strong>に，これまでとはまったく別の角度からアタックした論文です．</p>

<p>PCA,ICAというと，特徴量抽出とか信号分離などによく使われているイメージですね．多くの「なんとかComponent Analysis」はデータの成分がある種の確率分布(多くの場合，正規分布)にしたがって生成されているという仮定に立脚しています．また，見つけることのできる成分（基底といいます）は線形なものに限られることがほとんど．</p>

<p>対してこの論文では，<strong>「多項式」で表現されるような基底を抜き出す</strong>ことに焦点を当てています．無理やり1ページで説明しようとするとこんな感じ．</p>

<p><img class="center" src="/images/VCA.png" width="650"></p>

<p>で，興味をもっていただけましたらスライドを眺めていただきたいのですが，この「多項式集合」を特徴量抽出に使うと，いい感じに<strong>組み合わせを考慮した素性を抽出</strong>することができるようです．しかも，（※多少の前提条件は必要ですが）この方法で抽出した特徴ベクトルを分類問題に適用すると，<strong>線形分離できることが保証される</strong>，というありがたさ．多項式カーネルを直接用いる分類に比べ，精度がほとんど落ちることなく，分類において最大100倍ほどの高速化を達成しています．</p>

<p>今後もいろいろな展開が考えられる，読んでいて楽しい論文でした．手法や達成した性能自体が凄い，というよりは，これまで機械学習で殆ど用いられてこなかった<strong>代数幾何の概念を完成度の高い形で既存の問題設定に組み込んだ</strong>．というのがBest Paperとして評価された一因なのかなとも思います．</p>

<p>ICML読み会での発表後にTwitterを眺めていたら，</p>

<p><blockquote class="twitter-tweet"><p>さっきグレブナー基底とか何とか言ったけど、これはむしろ自由度の高い単なる多項式回帰な気がするので、今からちゃんと読むです… [要出典]</p>&mdash; ぽよ子 (@tara_nai) <a href="https://twitter.com/tara_nai/statuses/354593532855590912">July 9, 2013</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<p>とのご指摘が・・・．これはすずかけ台の勉強会でも指摘されて，ちょっとアレレってなっていた部分でもありました．説明できなくてスミマセン．</p>

<p>未だにアレレってなっているのですが，たしかに問題自体は多項式回帰の枠組みで考えることもできそうです．ただ，効率的にかつ冗長性を抑えながら，できるだけ小さい次数からFilterしていくということを考えると，イデアルのもつ吸収律を活用する，VCAのような枠組みが有効に働いてくるのではないかと思います．（スミマセン，イデアルとはいったい何なのか，グレブナー基底とはいったい何なのか，未だにあんまりわかってないので自信はありません・・・）</p>

<p>ちなみにこのチーム，すでに<strong>Deep Learningへの応用</strong>を考えていらっしゃるみたいで，そちらの<a href="http://arxiv.org/abs/1304.7045">プレプリント</a>も出てたりします．なんともはや．</p>

<p>他の方の発表も興味深い話が盛りだくさんで，すずかけの方では元同期の <a href="http://twitter.com/tma15">@tma15</a> くんが<a href="http://tma15.github.io/blog/2013/7/it-takes-a-long-time-to-become-young.html">オンラインコミュニティにおける参加者の寿命を言語的特徴から推測する話(WWWのベストペーパー)</a>を紹介してくれたり，ICML読み会のほうは．</p>

<ul>
<li>超多クラスのロジスティック回帰の分散学習</li>
<li>オンラインのマルチタスク学習(Lifelong Learningというらしいです)の爆速化</li>
<li>重みベクトルのビット数をAdaptiveに調節して省メモリに</li>
<li>ローカルには線形なSVMを多数組み合わせることで非線形分類を高速化</li>
<li>Deep LearningにおけるRectifierに代わる活性化関数Maxoutの提案</li>
<li>高次の共起を考慮にいれたタグの補完に基づいた高速画像タギング</li>
<li>ビデオ映像からの人間の動作認識</li>
<li>複数の目的関数をパレート最適に持っていくようなActive Learning</li>
<li>あんまり確率的ではないTopic Modelとその特徴付け</li>
</ul>


<p>などなど（発表の要旨をつかめてなかったらすみません），何でもありな感じでワクテカ．実際の現場で機械学習を応用されてるPFIの方の発表が多かったこともあり，<strong>精度を下げずにいかに高速化するか</strong>，という方向性の論文が多かったように思います．</p>

<p>個人的には， @sla さんがオープニングで話してくださった，「<strong>なんでもi.i.dを仮定するのはそろそろ脱却して，時間/空間に沿って変化するような動的なデータに対しても理論を作っていくべき</strong>」というコメントがたいへん興味深く感じました．</p>

<p>機械学習が実世界に浸透していくにしたがって，これからはひとつのモデルなり分類器なりがそれなりに長く使われることを考えなければならない時期にきているのかなとも思います．そういう状況のもとでは，(たとえば言語の話でいえばその時々で流行語があるように)<strong>入力の分布がどんどん変わっていく</strong>ような状況に対してうまく適応していくような問題設定を明確に意識していく必要がありそうです．
<a href="http://twitter.com/unnonouno">@unnonouno</a> さんが紹介してくださった，Lifelong Learningも，そういうモチベーションを含んでいるのかな．</p>

<p>私個人としましては，もう長いこと途絶えてしまっている外部へのアウトプットを再開するよい機会になりましたので，今後は知識を頂くだけではなく，自分の学んだことを共有することを心がけていきたいと感じた会でした．</p>

<p>久しぶりにお会いした方もいらっしゃったので，ゆっくりお話できれば良かったのですが，ちょっと事情がありまして早々と帰宅．また近いうちにお会いできる機会があれば幸いです（人生相談にのっていただきたいのでした・・・）</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Programming by Exampleに対する機械学習からのアプローチ（あるいは，「重い」処理を機械学習で「軽く」する，という視点）について]]></title>
    <link href="http://conditional.github.io/blog/2013/06/01/machine-learning-framework-for-programming-by-example/"/>
    <updated>2013-06-01T15:26:00+09:00</updated>
    <id>http://conditional.github.io/blog/2013/06/01/machine-learning-framework-for-programming-by-example</id>
    <content type="html"><![CDATA[<p>およそひと月ぶりに，仲間内で行っている小さな勉強会で論文紹介をしてまいりました．ICML2013の予稿がちょっとづつ出てきているので，本日はその中から一本．</p>

<ul>
<li><a href="http://jmlr.csail.mit.edu/proceedings/papers/v28/menon13.pdf">"A Machine Learning Framework for Programming by Example"</a> Aditya Menon et al, ICML 2013</li>
</ul>


<p><iframe src="http://www.slideshare.net/slideshow/embed_code/22279251" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen webkitallowfullscreen mozallowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="http://www.slideshare.net/koji_matsuda/a-machine-learning-framework-for-programming-by-example" title="A Machine Learning Framework for Programming by Example" target="_blank">A Machine Learning Framework for Programming by Example</a> </strong> from <strong><a href="http://www.slideshare.net/koji_matsuda" target="_blank">koji_matsuda</a></strong> </div></p>

<p>機械学習を使って，Programming by Example(PbE)をしようという論文です．PbEというのは私も初耳だったのですが，ざっくり言うと，<strong>人間が「例」を与えることで，その例をうまく再現するようなプログラムを自動的に生成する</strong>，というタスクのようです．</p>

<p>それを部分的に実現している（らしい）のが，Excel2013に新しく搭載されたという<a href="http://blogs.office.com/b/microsoft-excel/archive/2012/08/08/flash-fill.aspx">"Flash Fill"</a>という機能．<strong>ユーザーの意図を推測して，ルールにしたがったセルの補完</strong>なんかをしてくれるみたいです．百聞は一見にしかず，以下の動画をご覧ください．</p>

<p><iframe width="480" height="270" src="http://www.youtube.com/embed/YPG8PAQQ894?feature=oembed" frameborder="0" allowfullscreen></iframe></p>

<p>Flash Fillは一行から一行（というよりは，セルからセル）への変換のみがターゲットでしたが，本論文では<strong>集計であるとか，重複の削除のように，複数の行にまたがった処理</strong>をうまく解く枠組みを提案しています．</p>

<p>具体的には以下の例が分かりやすいかもしれません．</p>

<p><img class="center" src="/images/programming_by_example_menon2013.png" width="650"></p>

<p>「書き換え前」「書き換え後」の文字列を与えると，それらをうまく繋ぐようなプログラムを生成する，というイメージです．Perlいらず？</p>

<p>手法としてはさほど難しいことはしておらず，無理やり一言でまとめてしまうと，<strong>「書き換えプログラム」の生成モデルをPCFGでモデル化して，それぞれの生成ルールの確率をlog-linearで書き，そのパラメータをPCFGのn-best導出を延々と求めながら更新していく</strong>，というだけ．見方によっては，あまりICMLっぽくは無いかも．</p>

<p>もちろん，書き換えルールの元になるような基礎的な関数セット(最低限チューリング完全なものがあれば原理的には可能ですが，現実的には重複除去とかカウントとかを用意しておくみたいです)はあらかじめ定義しておかなければならないのですが，あとは<strong>探索時間さえ掛ければ任意の書き換えを行うプログラムが生成可能</strong>である，というのがポイントです．</p>

<p>ただ，現実的な時間で探索を行うためには<strong>探索に優先順位</strong>をつける必要があります．そこで，効率的に探索を行うために<strong>機械学習を用いて「書き換え」の集合をあつめたコーパスからPCFGのパラメータを学習</strong>しましょう，というのがこの論文の本旨のようです．実際，探索が難しいプログラムほど高速化が期待でき，brute forceな方法にくらべると40倍ほど速くなる場合もあるとのこと．</p>

<p>機械学習は「適応的なシステム」を作るのに使われる「重い」手法，というイメージがありますが，<strong>もともと原理的に解ける問題を「高速化」するために用いる</strong>，という意味で面白い視点を与える論文だと思いました．</p>

<p>ビッグデータ感もなければ，ディープラーニングでもなく，曼荼羅のようなグラフィカルモデルもなんとかダイバージェンスも出てこない論文ですが，なかなか面白かったです．実用に近そうなのもいい．</p>

<p>話は変わりまして，先日大学の同期とのプチ同窓会で10人ばかり集まったのですが，誰一人結婚してなくて笑．もちろん，ある種の集合バイアスもあるとは思いますが，おいおい俺らそろそろみそじやで．</p>
]]></content>
  </entry>
  
</feed>
