<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: study | a lonely miner]]></title>
  <link href="http://conditional.github.io/blog/categories/study/atom.xml" rel="self"/>
  <link href="http://conditional.github.io/"/>
  <updated>2013-09-08T18:31:45+09:00</updated>
  <id>http://conditional.github.io/</id>
  <author>
    <name><![CDATA[Koji Matsuda]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[距離計量学習とカーネル学習について]]></title>
    <link href="http://conditional.github.io/blog/2013/04/20/distance-metric-learning-and-kernel-learning/"/>
    <updated>2013-04-20T15:53:00+09:00</updated>
    <id>http://conditional.github.io/blog/2013/04/20/distance-metric-learning-and-kernel-learning</id>
    <content type="html"><![CDATA[<p>こんにちは．英語が書けなくて悩んでいる今日このごろです．</p>

<p>先月に引き続き，仲間内で行っている小さな勉強会にて論文紹介をしてまいりました．</p>

<ul>
<li><a href="http://dl.acm.org/citation.cfm?id=1273523">"Information-Theoretic Metric Learning"</a> V. Davis et al, ICML 2007 (Best Paper)</li>
</ul>


<p>ちょっと古めの論文ですが，あまり踏み込んだことのない分野なので，名著っぽいものから確実におさえていくスタンスで．</p>

<p>発表スライドは以下においておきます．最後のスライドにいろいろ文献リンクしておいたので，ご興味をもって頂けましたら是非そちらも当たってみてください．</p>

<p><iframe src="http://www.slideshare.net/slideshow/embed_code/19254185" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen webkitallowfullscreen mozallowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="http://www.slideshare.net/koji_matsuda/informationtheoretic-metric-learning" title="Information-Theoretic Metric Learning" target="_blank">Information-Theoretic Metric Learning</a> </strong> from <strong><a href="http://www.slideshare.net/koji_matsuda" target="_blank">koji_matsuda</a></strong> </div></p>

<p>距離計量学習(以下単に距離学習)とは何ぞや，というのは<a href="http://d.hatena.ne.jp/mamoruk/20090126/p1">小町さんの日記</a>をご参照いただけると良いと思うのですが，
ざっくり言うと，「分類しやすいように前処理として空間を歪めてしまおう」という技法です．ケーキで表す（！）と以下のような感じ．マシンラーニングケーキかっこいい！</p>

<blockquote><p> <a href="http://www.cse.wustl.edu/~kilian/code/page21/page21.html">Large Margin Nearest Neighbors</a></p>

<p><img src="http://www.cse.wustl.edu/~kilian/code/page21/files/img_3825_2.jpg" width="480" height="437"></p></blockquote>

<p>要するに，同じクラスの事例同士は近く，異なるクラスの事例同士は遠くなるように，元の空間を歪めてしまうのです．</p>

<p>この論文では，距離学習を<em>多変量正規分布間の KL Divergence</em> (の特別な場合である <em>LogDet Divergence</em> )の最適化問題として定式化するとともに，<em>カーネル学習との等価性</em>について述べています．実際のアルゴリズムは，<em>Bregman Projection</em>とか名前はごっついけど実際はそんなに難しくない．</p>

<p>確かに，カーネル行列というのは，バラすと特徴量空間での距離を畳み込んだものと言えますし，関連はありそうだと思ったものがきれいに証明されていてけっこう感動しました．</p>

<p>さらには，(正直あまり理解できませんでしたが)元の距離学習のカーネル化やオンライン化（Regret Boundまで！）などなど，8ページギッシリ詰められています．</p>

<p>ただ，学習されるのは単なるマハラノビス距離行列という<em>単なる線形変換</em>なので，どれほどのタスクで効くのかどうかは疑問が残ります．</p>

<p>たとえば，<em>元の空間で線形分離不可能な問題というのは，どんな線形変換を施したとしても，きれいに線形分離できるようにはならない</em>でしょうし，特徴空間での回転（マハラノビス距離行列の非対角要素）は何を表しているのか正直よくわかりません．</p>

<p>もっとも，このアルゴリズムはカーネル化ができることが示されているので，分離しやすい空間へ飛ばしてから距離学習を行えば別なのかもしれませんけど・・・けど・・・けど・・・．</p>

<p>こういう点では，多様体学習（詳しくないのですが，たとえばLaplacian Eigenmapsとか）のような非線形のアプローチのほうが，もともとの目的（分類しやすいように前処理する）に合っているように感じます．</p>

<p>計算量的な問題でいろいろ難しいのかもしれませんが，こういうSupervisedな距離学習あるいは次元削減で，オススメの文献がありましたら是非教えてください．</p>

<p>は〜英語書きに戻ろう．実際に書いている時間なんてほんの僅かで，書けないよーってうんうん呻ってる時間がほとんどなのだけれど．</p>

<p>そういえば，そういえば，一昨日第一回コンペが終わったCrowdSolvingについてはどれくらい書いていいのかな？いずれにしても，また次回．</p>

<ul>
<li>4/22 14:00 論文等へのリンクが間違っていたので修正しました．</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[グラフ結合度に基づく教師なし語義曖昧性解消について]]></title>
    <link href="http://conditional.github.io/blog/2013/04/01/unsupervised-wsd-with-graph-connectivity/"/>
    <updated>2013-04-01T13:13:00+09:00</updated>
    <id>http://conditional.github.io/blog/2013/04/01/unsupervised-wsd-with-graph-connectivity</id>
    <content type="html"><![CDATA[<p>完全に春ですね．そろそろ新入生が来る時期．</p>

<p>書類を整理していたら，むかし読んだ Roberto Navigli (knowledge based WSDの大家) によるgraph based unsupervised WSDに関する論文の感想が出てきたのでちょっと再編集して公開してみます．あくまでメモなので，非常に読みづらいかも．もしご興味をもって頂けましたら，元論文をあたってみてください．</p>

<ul>
<li><a href="http://wwwusers.di.uniroma1.it/~navigli/pubs/PAMI_2010_Navigli_Lapata.pdf">R. Navigli, M. Lapata. An Experimental Study of Graph Connectivity for Unsupervised Word Sense Disambiguation. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 32(4), IEEE Press, 2010, pp. 678-692.</a></li>
</ul>


<p>グラフの G = (V, E) をコンテキストおよび、ワードネットのネットワークから作成し，その上で 各語義 (vertexに対応)の重要度をはかるという方法に基づいたWSDに関するお話．</p>

<p>具体的には、まずコンテキスト（本論文では文）内の単語の各語義をグラフのノードとして初期化し、それぞれの語義からグラフ内のほかの語義へのリンクをDFSで探し，みつかればそのルート中の語義をグラフに追加していき，その間にエッジを張る．ということを行っている．</p>

<p>この操作で出来たグラフの「あるノード」の重要度が高いようであれば，その語義はコンテキスト内での重要度が高く，語義候補である可能性が高い，という直感に基づいている．</p>

<p>グラフは以下の図のような感じ．これは，{動詞drink, 名詞milk}の例．それぞれ5つ，4つの語義が定義されていて，それらが 名詞boozing_1, 名詞beverage_1 ような語義を通して間接的に連結されていることがわかる．</p>

<p><img src="http://conditional.github.com/images/graph-based-wsd.png" width="480"></p>

<p>肝心の重要度尺度であるが，彼らは local な尺度と globalな尺度という，大きく分けて二種類の尺度について実験を行っている．</p>

<p>localな尺度を用いた手法では、グラフ内のノードをPagerankや度数といった指標に基づいてランキングし、もっともSenses(wi)に対応するノードの中で最も高いランクを得た語義を出力．こっちは簡単．</p>

<p>globalな尺度はグラフ全体に対してスカラーの値を与えるものなので，語義選択においてはそのままでは用いることができない．そこで， G の部分集合となるような G'(コンテキスト内の単語それぞれについて語義ひとつだけを考えたもの) を考え，そのサブグラフのglobal measureの値が高いものを語義の組み合わせとして導きだす． たとえば，文が 2単語から成っており、それぞれが， 3つ，4つの語義を持っている場合は、3 *4 で12個のサブグラフを作り，それぞれに対して global measureを計算する。もっとも高いglobal measueを得たサブグラフを語義の組み合わせとして出力する．（計算量が爆発するという意味でオリジナルのLeskアルゴリズムと類似している）</p>

<p>過去の研究として，</p>

<ul>
<li>R. Barzilay and M. Elhadad, “Using Lexical Chains for Text Summarization,” Proc. ACL Workshop Intelligent Scalable Text Summarization, pp. 10-17, 1997.</li>
<li>R. Mihalcea, “Unsupervised Large-Vocabulary Word Sense Disambiguation with Graph-Based Algorithms for Sequence Data Labeling,” Proc. Human Language Technology and Empirical Methods in Natural Language Processing, pp. 411-418, 2005</li>
<li>M. Galley and K. McKeown, “Improving Word Sense Disambiguation in Lexical Chaining,” Proc. 18th Int’l Joint Conf. Artificial Intelligence, pp. 1486-1488, 2003.</li>
</ul>


<p>が比較に挙げられているが，本論文の手法は one sense per one discourse を強く仮定しない(同じドキュメント内で同じ対象語に対してもコンテキストが異なれば違う語義が出力されうる)．また、グラフをunlabeled &amp; unweightedに構築している点が異なる．これには以下の二つの理由があるらしい．</p>

<ul>
<li>広く合意を得たweightingの方法が確立されていない</li>
<li>研究の焦点を絞りたい(まぁweightingは今のところ特に興味ない)</li>
</ul>


<p>また、WordNet にくっついている MFS(というより，語義頻度) は本論文ではつかっていない．なんでかというと，それはhand-labeledなSemcorコーパスから得られたものであり，他の言語とかドメインに対しても有効な指標とは言えないから（この点は激しく同意）．</p>

<p>以下，それぞれの尺度についてまとめる．</p>

<h3>local measure</h3>

<p>local measureは特定のグラフのノードの重要度を表すもの．あるノードのグラフ全体に対する影響度，とみなすこともできる 値域は [0,1] で、1に近ければ重要、0に近ければ重要ではない．</p>

<ul>
<li><p><strong>Degree</strong> これは単なる次数。deg(v) = vの次数とすると， C_degree(v) = deg(v) / (|V| - 1)というように正規化しておく</p></li>
<li><p><strong>Eigenvector</strong> ようは PagerankとHITS．特筆すべきことはなし．</p></li>
<li><p><strong>KPP</strong>（あとで読む)</p></li>
<li><p><strong>Betweeness</strong> shortest pathの数を用いる． σ<em>st = s->tへのshortest pathの数，　σ</em>st(v) そのうち、vを通るもの．σ<em>st(v) / σ</em>st をすべてのノードペアに対して和をとって，betweeness(v)とする．そして(|V| - 1)(|V| - 2)で割って正規化．</p></li>
</ul>


<h3>global measure</h3>

<p>さきほどのlocal measureがグラフのノードに対して重要度を与えるものだったのに対して，こちらは特定の語義の組み合わせからなるグラフに対して[0,1]のスカラー値を与えるもの．</p>

<ul>
<li><strong>Compactness, Graph entropy, Edge density</strong> いわゆるグラフのコンパクト性などの一般的な尺度</li>
</ul>


<p>global measureは計算量的に無理がある（文内のすべての単語に対する語義の組み合わせを列挙してそれぞれに対して求めなければならない)ので工夫をしている．</p>

<h4>global measure計算における工夫</h4>

<ul>
<li><p><strong>Simulated Annealing</strong> まずランダムに語義選択を初期化，ひとつ取りかえて上記 global measure の差分(ΔE)をみる．もしよくなっていれば採択、悪くなっている場合でも exp(ΔE/T)の確率で採択．Tは何らかの定数，これを u 回繰り返した結果を採用．書いてて思いましたが，Simulated Annealingというよりは，Metropolis Hastingsですねこれ．</p></li>
<li><p><strong>Genetic Algorithms</strong>
なんか面倒そうなので略．結局パラメータ調整は面倒だしあんまりいいこと無い，みたいな結論に至っており，若干残念な感じ．</p></li>
</ul>


<!--
### Complexity

まず初期グラフのノード数について，

k = WordNet内の語の最大の語彙数(最も多い語義を持つ語の語義数)
n = sentence σの長さ．
|V_σ| は knのオーダーだけど、もう少しタイトに上界を抑えることはできるようだ

グラフ作成には、だいたい O(n^2
)くらい。
各尺度の計算量については，論文中のTable3に書いてある．
-->


<h3>Experiment</h3>

<p>データはSemCor, Senseval-3, Semeval-2007．Sensemapつかって全部WordNet2.0にマップしてある．</p>

<p>Sense-inventoryにはWordNet2.0 と EnWordNetというものを使っている．EnWordNetはcollocational relationをもちいてedgeを6万本くらい増やしたWordNetらしい．具体的な構築方法はよく分からないが． WN++みたいなものか．</p>

<p>ふつうのWordNetから構築したグラフと，EnWordNetから構築したグラフの性質の比較も行っている．</p>

<ul>
<li><p><strong>small world effect</strong> : l = グラフのスモールワールド性（任意の頂点ペアの間の最短距離の平均)</p></li>
<li><p><strong>clustering rate(or transitivity)</strong> : C = probability that two neighbors of a vertex are connected 詳しくは式参照 ネットワーク内の三角形の数で計算するらしい。</p></li>
<li><p><strong>cumulative degree distribution</strong> : P_k = Σ p_k' (&lt;- k次のノードの割合)</p></li>
</ul>


<p>結果として，EnWordNetから構築したグラフのほうがdenseでござるという当たり前の議論が．(そりゃあエッジ足してるだけなのだから当然)</p>

<p>グラフ生成におけるDFSの深さは6にしている（SemCor上で実験して決めたらしい)</p>

<p>その他、GA, SAのパラメータについてうんたらかんたら．</p>

<h4>Result</h4>

<p>Degree（もっとも単純なlocal measure,ノードの次数そのもの) がもっともよく，いろいろ工夫した他のlocal measureやglobal measureは振るわなかった．残念．．．</p>

<p>WordNet vs EnWordNetの比較では，EnWordNetの方が若干ながら良い結果．</p>

<p>しかしながら，unsupervised WSDの常として，MFS(First Sense Baseline)は非常に強く，F1 measureで20ポイント以上の差をつけられてしまっている．</p>

<h3>感想</h3>

<p>グラフとしては最も基本的な， unlabeled, undirected, unweightedなものの上で何ができるか，を追求した研究といえる．</p>

<p>単純な指標がうまくいってしまい，いろいろ工夫しても無駄っぽいのは残念だが，unsupervised WSDの難しさは語義粒度とknowledge baseの質如何でいくらでも変動しうるので，言語資源の整備が進めば，また違った結果が得られるかもしれない．</p>

<p>WSDにおいて宿命になっている，"MFSに勝てない問題"については，近年は</p>

<ul>
<li><a href="http://www.aclweb.org/anthology/P/P10/P10-1154.pdf">SP Ponzetto, R Navigli "Knowledge-rich Word Sense Disambiguation Rivaling Supervised Systems" Proc of ACL 2010</a></li>
</ul>


<p>のように，知識ベース(WordNet)側をガンガン強化することで，ほぼ遜色ない性能を出すことができる手法も出てきている．しかし，こちらも，もっとも性能の良い尺度はDegreeなので，なんだかなぁという気はする．</p>

<p>結局知識ベースの品質に強く依存するknowledge based WSDではあるが，まだ何かできることは無いかと考えると，うまくいくかはともかくとして，いろいろ面白そうだ．（ニッチなためか，機械学習屋さんがまだあまり進出してこない分野でもあるので）</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Posterior Regularization と Unified Expectation Maximizationについて]]></title>
    <link href="http://conditional.github.io/blog/2013/03/23/about-posterior-regularization-and-unified-expectation-maximization/"/>
    <updated>2013-03-23T16:20:00+09:00</updated>
    <id>http://conditional.github.io/blog/2013/03/23/about-posterior-regularization-and-unified-expectation-maximization</id>
    <content type="html"><![CDATA[<p>桜がとってもきれいですね．すずかけ台は8分咲きといったところです．ところで，仲間で行っている小規模な勉強会で</p>

<ul>
<li><a href="http://www.aclweb.org/anthology/N/N12/N12-1087.pdf">"Unified Expectation Maximization"</a> Samdani et al, NAACL2012</li>
</ul>


<p>を紹介してきたので，資料をslideshareにあげておきました．</p>

<p><iframe src="http://www.slideshare.net/slideshow/embed_code/17552885" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen webkitallowfullscreen mozallowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="http://www.slideshare.net/koji_matsuda/unified-expectation-maximization" title="Unified Expectation Maximization" target="_blank">Unified Expectation Maximization</a> </strong> from <strong><a href="http://www.slideshare.net/koji_matsuda" target="_blank">koji_matsuda</a></strong> </div></p>

<p>Unified EMというと，じゃっかん大風呂敷な感じのタイトルですが，キーとなるアイデアはとても単純で，EMアルゴリズムのE-Stepで最小化するKLダイバージェンスにちょっと細工を入れることで，Hard-EMとふつうのEMの中間くらいの性質を持ったアルゴリズムになりますよ．というお話です．Deterministic Annealing EMの逆バージョンみたいな雰囲気(実際，DAEMもこの枠組で書けることが示されています) 手元にEMのコードがあれば，実装も非常に容易．</p>

<p>ただ，やっぱりそれだけだと一発ネタにしかならないので，「制約付きEM」のほうへ話を進めています．「制約付きEM」というと聞きなれないアルゴリズムですが，Un(semi-)supervised learningにおいて，事前知識を用いてモデルがとんでもない方向へ飛ぶのを防ごう，というモチベーションに基づく技法のようです．</p>

<p>自然言語処理におけるアプリケーションでは，たとえば以下のような制約を考えることができます：</p>

<ul>
<li><p>品詞タグ付けなら</p>

<ul>
<li>ある文には，名詞と動詞が最低一つづつ含まれる</li>
<li>ある語が，複数のPOSに割り当てられることは稀</li>
</ul>
</li>
<li><p>機械翻訳におけるアラインメントなら</p>

<ul>
<li>L1->L2のアラインメントと，L2->L1のアラインメントは一致する</li>
<li>L1の一つの語が，L2の多数の語と対応付けられることは稀</li>
</ul>
</li>
<li><p>関係抽出なら</p>

<ul>
<li>ある種のエンティティと，ある種のエンティティの間には，特定のリレーションしか成り立たない

<ul>
<li>(PERSON, LOCATION) -> LIVE IN</li>
<li>(ORGANIZATION, PERSON) -> WORK FOR</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>こういった事前知識に基づく制約を満たすモデルを，確率分布の集合として表現し，そこから離れないようにEMアルゴリズムを行うことによって，ラベルつきデータが利用可能ではない（または，少量しか存在しない）状況において，うまく学習が行おうというのが，「制約付きEM」の肝となる部分です．直感的には，Posterior Regularizationの論文から引用した以下の図が分かりやすいかもしれません．（日本語注釈は私によるものです）</p>

<p><img src="http://conditional.github.com/images/posterior_regularization_fig.png" width="460" height="302"></p>

<p>歯切れのよいタイトルに惹かれて軽い気持ちで選んだ論文でしたが，そこそこホットな分野のようで，ACL 2011のチュートリアルで1トラックまるまるこの話題だったりしたらしく，問題設定や前提を理解するのにけっこう苦労しました．</p>

<p>結果として，UEMの本題ではなく，問題設定や先行研究の紹介に半分近くのスライドを割くことに・・・．まぁ，楽しんで頂けたようなのでなによりです．しかしひさびさに緊張感のあるプレゼンだった．</p>

<p>最後のスライドにも記載しましたが，以下の文献が，理解の助けになると思います．</p>

<ul>
<li><a href="http://sideinfo.wikkii.com/">"Rich Prior Knowledge in Learning for Natural Language Processing"</a> ACL 2011 tutorial

<ul>
<li>ACLで行われたチュートリアルの資料です．このファミリーに属するアルゴリズムについての資料の多くはここからたどれるようになっています．各種のアルゴリズムのあいだの関係についてよくまとまっていますし，著者らによって異なる表記系もすっきりまとめられているので読みやすい．今回は取り上げませんでしたが，Labeled Feature とかも追ってみるとおもしろそうなトピックなので，ぜひ．</li>
</ul>
</li>
<li><a href="http://jmlr.csail.mit.edu/papers/volume11/ganchev10a/ganchev10a.pdf">"Posterior Regularization for Structured Latent Variable Models"</a> JMLR 2010

<ul>
<li>今回紹介した論文の元ネタになっている Posterior Regularizationの論文です．品調ラベルづけにおける例とともに，少しづつ丁寧に議論が進められており，さすがジャーナルだけあって読みやすいです．</li>
</ul>
</li>
</ul>


<p>ひさびさに負荷の高い一週間だったので，来週はすこしゆっくりしたいと思います．（日記）</p>
]]></content>
  </entry>
  
</feed>
