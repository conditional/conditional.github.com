
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>[MLAC 2013 7日目] Torch7でお手軽ニューラルネットワーク - a lonely miner</title>
  <meta name="author" content="Koji Matsuda">

  
  <meta name="description" content="はじめに この記事は Machine Learning Advent Calendar 2013 の 7日目の記事です． 2013年，Deep Learning もアカデミックレベルではさまざまな分野への浸透が進み，バズワードの域を脱したように思えます．
これまでは，機械学習というと， &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://conditional.github.io/blog/2013/12/07/an-introduction-to-torch7">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="a lonely miner" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-39246182-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">a lonely miner</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:conditional.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">[MLAC 2013 7日目] Torch7でお手軽ニューラルネットワーク</h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-12-07T21:29:00+09:00" pubdate data-updated="true">Dec 7<span>th</span>, 2013</time>
        
      </p>
    
  </header>


<div class="entry-content"><h2>はじめに</h2>

<p>この記事は <a href="http://qiita.com/advent-calendar/2013/machinelearning">Machine Learning Advent Calendar 2013</a> の 7日目の記事です．</p>

<p>2013年，<em>Deep Learning</em> もアカデミックレベルではさまざまな分野への浸透が進み，バズワードの域を脱したように思えます．
これまでは，機械学習というと，応用分野においては(分類/回帰といった)タスクを決めてしまった上でブラックボックスとして
扱うもの，という空気がありましたが，
<em>Deep Learning</em> に代表される柔軟な，いかようにも組み上げられるモデルは，問題の性質を積極的に(特徴量としてではなく，モデル自体に)組み込むことを容易にする，大きな武器になるのではないかと感じています．</p>

<p>素性エンジニアリング vs モデルパラメータエンジニアリング の不毛な戦いが幕を上げた，という見方もできちゃいそうですが・・・．．</p>

<p>さて今回は， <a href="http://torch.ch/">Torch7</a> という，Neural Networkを中心とした機械学習向けの環境をごくごく簡単に紹介します．
Torch7自体は，比較的最近公開されたソフトウェアですが，”7&#8221;という文字から伺えるとおり，
主開発者の Ronan Collobert 氏が中心となって，かれこれ10年以上継続的に開発されているパッケージのようです．</p>

<p>論文としては， <a href="http://data.neuflow.org/pubs/biglearn11.pdf">NIPS’11 BigLearn Workshop のもの</a>が初出でしょうか．
Neural Network向けのソフトウェアパッケージというと，
<a href="http://deeplearning.net/software/theano/">Theano</a>/<a href="http://deeplearning.net/software/pylearn2/">Pylearn2</a> が主流ですが(要出典)，Torch7の特徴をいくつか挙げてみましょう．</p>

<ul>
<li>スクリプティング言語として， Lua が採用されています．
　そのため，他のアプリケーションにおけるスクリプティング環境としての組み込みが容易になっているそうです．
　試していないのですが，iOSアプリへの組み込みも可能だとか・・・？</li>
<li>テンソルのサポートが手厚く，次元を変えたりメモリ上の配置を変えたり，四則演算を行ったり，がらくちん</li>
<li>NNを構成するモジュールが大量に備わっており，ブロックを組み立てる感覚でモデルを作ることができる</li>
<li>独自のオブジェクトシステムが備わっており，OOPっぽいプログラムが可能</li>
<li>BLAS, CUDAなどのサポート</li>
<li><strong>なぜかIDEがついてくる！！</strong> (<code>torch -ide</code>で起動できます)</li>
</ul>


<p>Theano/Pylearn2との比較については，Theano開発チームによる以下の資料(とくに Table 1)が参考になります．</p>

<ul>
<li><a href="http://arxiv.org/pdf/1211.5590v1.pdf">Theano: new features and speed improvements</a></li>
</ul>


<p><img class="center" src="/images/torch/features.png" width="650"></p>

<p>Theano は Deep Learning の総本山であるモントリオール大のチームが強力に推進しているプロジェクトだけあって，
機能面での対抗はなかなか厳しそうですが，頑張ってます，，，頑張っています！</p>

<h2>Torch7のセットアップ</h2>

<p><a href="http://torch.ch/">Torch7</a> よりどうぞ．</p>

<p>インストールしたのがしばらく前なので記憶が曖昧なのですが，私の環境(MacOS X 10.9)では，特に引っかかるところはなかったように思います．要<code>cmake</code>．</p>

<p>私は使いませんでしたが，<code>apt</code>が動くLinuxか，<code>homebrew</code>が動くMacなら，インストールスクリプトも用意されています．
しかし，こちらのインストールスクリプト，若干お行儀が悪い気もするので，気になる方は実行前に一度眺めてみてください．</p>

<p>各種の拡張機能(カメラへのアクセスなど)は，<code>luarocks</code>というパッケージマネージャ(Rubyにおける<code>gem</code>,Pythonにおける<code>pip</code>のようなもの)を用いて管理するようです．</p>

<h2>Torch7で多層パーセプトロン</h2>

<h3>ネットワークの構築</h3>

<p>Torch7 における Neural Network の構築は非常に簡単です．
たとえば，基本的な中間層1層のネットワークは，以下のようなコードで表現できます．</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='lua'><span class='line'> <span class="nb">require</span> <span class="err">“</span><span class="n">nn</span><span class="s2">&quot;</span>
</span><span class='line'> <span class="n">mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>        <span class="c1">-- Multi Layer Perceptron</span>
</span><span class='line'> <span class="n">mlp</span><span class="p">:</span><span class="n">add</span><span class="p">(</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span> <span class="p">)</span> <span class="c1">-- 1000 input, 25 hidden units</span>
</span><span class='line'> <span class="n">mlp</span><span class="p">:</span><span class="n">add</span><span class="p">(</span> <span class="n">nn</span><span class="p">.</span><span class="n">Tanh</span><span class="p">()</span> <span class="p">)</span> <span class="c1">-- hyperbolic tangent transfer function</span>
</span><span class='line'> <span class="n">mlp</span><span class="p">:</span><span class="n">add</span><span class="p">(</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="p">)</span> <span class="c1">-- 10 output</span>
</span><span class='line'> <span class="n">mlp</span><span class="p">:</span><span class="n">add</span><span class="p">(</span> <span class="n">nn</span><span class="p">.</span><span class="n">SoftMax</span><span class="p">()</span> <span class="p">)</span> <span class="c1">-- softmax output</span>
</span></code></pre></td></tr></table></div></figure>


<p>このネットワークを，トレーニングデータに対する負の対数尤度を目的関数として訓練するには，</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='lua'><span class='line'> <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ClassNLLCriterion</span><span class="p">()</span>
</span><span class='line'> <span class="n">trainer</span>   <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">StochasticGradient</span><span class="p">(</span><span class="n">mlp</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
</span><span class='line'> <span class="n">trainer</span><span class="p">:</span><span class="n">train</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>とするだけです．明解ですね．</p>

<p><code>“nn"</code>パッケージには，約80種類のさまざまな Module (NNの層や目的関数に相当)が用意されており，これらを自由に組み合わせてネットワークを作ることが可能です．
行う価値や効率的な最適化法があるかどうかは別にして， <code>add()</code> を用いてどんどん繋げていけばいくらでも <em>Deeeeeeeeeep</em> にすることも簡単にできそうです．</p>

<h3>学習過程の制御</h3>

<p>基本的には以上なのですが，最近の Neural Network の訓練で必須になってきている mini-batch や momentum のような技術を用いたり，
準ニュートン法や共役勾配法などのより高度な最適化を行うために，より細かく学習の制御を行う方法も用意されています．</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='lua'><span class='line'> <span class="n">parameters</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">mlp</span><span class="p">:</span><span class="n">getParameters</span><span class="p">()</span>
</span><span class='line'> <span class="n">feval</span> <span class="o">=</span> <span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span class='line'>           <span class="c1">-- input : 入力事例テンソル , target : inputに対する正解ラベル(一般にはテンソル)</span>
</span><span class='line'>           <span class="kd">local</span> <span class="n">output</span> <span class="o">=</span> <span class="n">mlp</span><span class="p">:</span><span class="n">forward</span><span class="p">(</span><span class="n">input</span><span class="p">)</span>     <span class="c1">-- 前向きに伝播させて，出力を得る</span>
</span><span class='line'>           <span class="kd">local</span> <span class="n">f</span>      <span class="o">=</span> <span class="n">criterion</span><span class="p">:</span><span class="n">forward</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="c1">--　決められた Criterion の元での目的関数の値を計算</span>
</span><span class='line'>           <span class="kd">local</span> <span class="n">df</span>     <span class="o">=</span> <span class="n">criterion</span><span class="p">:</span><span class="n">backward</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="c1">-- 勾配を計算</span>
</span><span class='line'>           <span class="k">return</span> <span class="n">f</span><span class="p">,</span> <span class="n">df</span>   <span class="c1">-- 目的関数の値と，その勾配を返す</span>
</span><span class='line'>         <span class="k">end</span>
</span><span class='line'> <span class="n">optim</span><span class="p">.</span><span class="n">lbfgs</span><span class="p">(</span><span class="n">feval</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span> <span class="c1">-- parametersを更新</span>
</span><span class='line'> <span class="c1">--（注意）細かい部分を省いた擬似コードなので，このままでは動かないと思います XD</span>
</span></code></pre></td></tr></table></div></figure>


<p>というように，現在のパラメータの元での目的関数の値と，その勾配を返すようなクロージャを作ることで，
 <code>“optim”</code> パッケージで用意されているさまざまな最適化アルゴリズム(SGD, AdaGrad, L-BFGSなど・・・)を用いることも可能です．</p>

<p>本稿では触れませんが，<code>module:forward()</code>, <code>criterion:backward()</code> でネットワークの出力，勾配を得て <code>module:gradUpdate()</code> で直接更新するという手段も用意されています．</p>

<h2>ためしてみる(数字認識タスク)</h2>

<p>ここでは，　<a href="http://code.cogbits.com/wiki/doku.php?id=start">Machine Learning with Torch7</a> で用いられているチュートリアルコードを実際に動かしてみます．</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='lua'><span class='line'> <span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">clementfarabet</span><span class="o">/</span><span class="n">torch</span><span class="o">-</span><span class="n">tutorials</span><span class="p">.</span><span class="n">git</span>
</span></code></pre></td></tr></table></div></figure>


<p>でお手元にクローンして， <code>2_supervised</code> の下にもぐってみてください．</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='lua'><span class='line'> <span class="n">torch</span> <span class="p">.</span><span class="o">/</span><span class="n">doall</span><span class="p">.</span><span class="n">lua</span>
</span></code></pre></td></tr></table></div></figure>


<p>でデータセットのダウンロードを含めた実験スクリプト全体の実行が始まります．初回は 300MB くらいのファイルをいきなり <code>wget</code> しはじめるのでご注意ください．</p>

<p>ここで扱っているのは， <a href="http://ufldl.stanford.edu/housenumbers/">Street View House Numbers (SVHN)Dataset</a>という，
カラー画像から数字を認識するタスク，Format 2(32x32の画像)のほうです．</p>

<p><img class="center" src="/images/torch/SVHN32x32eg.png" width="480"></p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='lua'><span class='line'> <span class="n">torch</span> <span class="p">.</span><span class="o">/</span><span class="n">doall</span><span class="p">.</span><span class="n">lua</span> <span class="o">-</span><span class="n">size</span> <span class="n">small</span> <span class="o">-</span><span class="n">model</span> <span class="n">linear</span> <span class="o">-</span><span class="n">batchSize</span> <span class="mi">100</span> <span class="o">-</span><span class="n">plot</span>
</span></code></pre></td></tr></table></div></figure>


<p>とかすると，ごく普通の線形モデルでの最適化が始まり，その過程が <code>gnuplot</code> でプロットされていきます．</p>

<p><code>2_model.lua</code> には線形モデル(<code>linear</code>),隠れ層1層のニューラルネット(<code>mlp</code>),<a href="http://arxiv.org/abs/1204.3968">2-stageのたたみこみニューラルネット</a>(<code>convnet</code>)
が定義されており， <code>-model</code> オプションで切り替えることが可能です．
おフロに入っている間，40分くらい回した結果は以下のような感じです．</p>

<ul>
<li><code>mlp</code>モデル</li>
</ul>


<p><img class="center" src="/images/torch/test_mlp_15epoch.png" width="480"></p>

<ul>
<li><code>convnet</code>モデル</li>
</ul>


<p><img class="center" src="/images/torch/test_convnet_13epoch.png" width="480"></p>

<p>うーん，<code>mlp</code>はまだまだ伸びそうだなぁ．<code>convnet</code>は局所解につかまってしまったのでしょうか．時間切れにつき，あまり直感に沿った結果は出せませんでしたが，学習によって正解率が向上している，ということは一応みてとれます．</p>

<h2>Torch7 をより深く知るために</h2>

<p>最後に，いくつか資料へのリンクをまとめておきます．</p>

<ul>
<li><a href="http://ronan.collobert.com/pub/matos/2012_implementingnn_springer.pdf">Implementing Neural Networks Eciently</a></li>
</ul>


<p>メイン開発者の Ronan Collobert 氏による Torch7 の解説です．ソフトウェア全体のアーキテクチャや， Lua を採用した理由，実際のパフォーマンスなどがまとめられています，</p>

<ul>
<li><a href="http://code.cogbits.com/wiki/doku.php?id=start">Machine Learning with Torch7</a></li>
</ul>


<p>上に挙げたチュートリアル資料です．Auto-Encoder のサンプルや，新しいモジュールのプログラミング方法など，本稿では触れられなかったトピックが盛りだくさんです．これを遊んでみるだけで年を越せるかも・・・</p>

<ul>
<li><a href="https://groups.google.com/forum/#!forum/torch7">torch7 - Google グループ</a></li>
</ul>


<p>流量はさほど多くありませんが，盛んに開発中のソフトウェアですので，こちらを追うことも重要かと思います．</p>

<ul>
<li><a href="http://www.slideshare.net/yurieoka37/ss-28152060">実装ディープラーニング - slideshare</a></li>
</ul>


<p>@0kayu さんによる，Pylearn2を中心としたディープラーニングの実装に関するよいまとめスライドです．</p>

<ul>
<li><a href="http://kiyukuta.github.io/2013/09/28/casualdeeplearning4nlp.html">自然言語処理まわりのDeep Learningを自分なりにまとめてみた — KiyuHub</a></li>
</ul>


<p>そもそもなんで自然言語処理においてニューラルネットが流行ってるの？というあたりに焦点を当てた資料です．ガッツリ！</p>

<h2>まとめ</h2>

<p>Lua を用いて Neural Network を簡単に構築できる， Torch7 というソフトウェアを紹介いたしました．</p>

<p>Theano ひとり勝ちというのはいかにもつまらないので，皆様にお手にとって頂くきっかけになれば幸いです．</p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Koji Matsuda</span></span>

      








  


<time datetime="2013-12-07T21:29:00+09:00" pubdate data-updated="true">Dec 7<span>th</span>, 2013</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/learning/'>learning</a>, <a class='category' href='/blog/categories/machine/'>machine</a>
  
</span>


    </p>
    
      <div class="sharing">
  <a href="http://b.hatena.ne.jp/entry/http://conditional.github.io/blog/2013/12/07/an-introduction-to-torch7/" class="hatena-bookmark-button" data-hatena-bookmark-layout="standard" title="このエントリーをはてなブックマークに追加"><img src="http://b.st-hatena.com/images/entry-button/button-only.gif" alt="このエントリーをはてなブックマークに追加" width="20" height="20" style="border: none;" /></a><script type="text/javascript" src="http://b.st-hatena.com/js/bookmark_button.js" charset="utf-8" async="async"></script>
  
  <a href="http://twitter.com/share" class="twitter-share-button" data-url="http://conditional.github.io/blog/2013/12/07/an-introduction-to-torch7/" data-via="conditional" data-counturl="http://conditional.github.io/blog/2013/12/07/an-introduction-to-torch7/" >Tweet</a>
  
  
  
    <div class="fb-like" data-send="true" data-width="450" data-show-faces="false"></div>
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2013/09/22/practical-recommendations-for-gradient-based-training-of-deep-architectures/" title="Previous Post: Deep Learning : Bengio先生のおすすめレシピ">&laquo; Deep Learning : Bengio先生のおすすめレシピ</a>
      
      
        <a class="basic-alignment right" href="/blog/2014/01/13/report-of-dsirnlp5/" title="Next Post: 「モデル」とは何か，について考えていたことを，DSIRNLP(データ構造と情報検索と言語処理勉強会)で発表してきました">「モデル」とは何か，について考えていたことを，DSIRNLP(データ構造と情報検索と言語処理勉強会)で発表してきました &raquo;</a>
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>About Me</h1>
  <p>ロンリーなマイナーです．</p>
  <ol>
  <li><a href="http://twitter.com/conditional">Twitter</a></li>
  </ol>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/01/13/report-of-dsirnlp5/">「モデル」とは何か，について考えていたことを，DSIRNLP(データ構造と情報検索と言語処理勉強会)で発表してきました</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/12/07/an-introduction-to-torch7/">[MLAC 2013 7日目] Torch7でお手軽ニューラルネットワーク</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/09/22/practical-recommendations-for-gradient-based-training-of-deep-architectures/">Deep Learning : Bengio先生のおすすめレシピ</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/09/08/report-of-snlp5/">第5回 最先端NLP勉強会に参加してきました</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/08/19/acl2013-reading-list-part-1/">ACL2013 マイ・リーディングリスト(1)</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/08/03/joint-modeling-of-a-matrix-with-associated-text-via-latent-binary-features/">コンピュータが政治をする時代(あるいは，行列とテキストの結合モデル)について</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/07/10/vanishing-component-analysis/">どんなデータでも(※)線形分離可能にしてしまう技術，Vanishing Component Analysis(ICML 2013)を紹介してきました</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/06/01/machine-learning-framework-for-programming-by-example/">Programming by Exampleに対する機械学習からのアプローチ（あるいは，「重い」処理を機械学習で「軽く」する，という視点）について</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/05/06/predicting-google-closures/">統計データに基づくGoogle各種サービスの生存予測</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/05/05/predicting-google-closures/">Predicting Google Closures</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/04/20/distance-metric-learning-and-kernel-learning/">距離計量学習とカーネル学習について</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/04/01/unsupervised-wsd-with-graph-connectivity/">グラフ結合度に基づく教師なし語義曖昧性解消について</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/03/23/about-posterior-regularization-and-unified-expectation-maximization/">Posterior Regularization と Unified Expectation Maximizationについて</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/03/14/take-a-nap-with-prowl/">Prowl+zshで快適お昼寝タイム</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/03/13/start-bloging-with-octopress/">Start Bloging With Octopress</a>
      </li>
    
  </ul>
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - Koji Matsuda -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'condi';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://conditional.github.io/blog/2013/12/07/an-introduction-to-torch7/';
        var disqus_url = 'http://conditional.github.io/blog/2013/12/07/an-introduction-to-torch7/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id; js.async = true;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>





  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
