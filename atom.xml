<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[a lonely miner]]></title>
  <link href="http://conditional.github.com/atom.xml" rel="self"/>
  <link href="http://conditional.github.com/"/>
  <updated>2013-04-01T14:35:32+09:00</updated>
  <id>http://conditional.github.com/</id>
  <author>
    <name><![CDATA[Koji Matsuda]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[グラフ結合度に基づく教師なし語義曖昧性解消について]]></title>
    <link href="http://conditional.github.com/blog/2013/04/01/unsupervised-wsd-with-graph-connectivity/"/>
    <updated>2013-04-01T13:13:00+09:00</updated>
    <id>http://conditional.github.com/blog/2013/04/01/unsupervised-wsd-with-graph-connectivity</id>
    <content type="html"><![CDATA[<p>完全に春ですね．そろそろ新入生が来る時期．</p>

<p>書類を整理していたら，むかし読んだ Roberto Navigli (knowledge based WSDの大家) によるgraph based unsupervised WSDに関する論文の感想が出てきたのでちょっと再編集して公開してみます．あくまでメモなので，非常に読みづらいかも．もしご興味をもって頂けましたら，元論文をあたってみてください．</p>

<ul>
<li><a href="http://wwwusers.di.uniroma1.it/~navigli/pubs/PAMI_2010_Navigli_Lapata.pdf">R. Navigli, M. Lapata. An Experimental Study of Graph Connectivity for Unsupervised Word Sense Disambiguation. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 32(4), IEEE Press, 2010, pp. 678-692.</a></li>
</ul>


<p>グラフの G = (V, E) をコンテキストおよび、ワードネットのネットワークから作成し，その上で 各語義 (vertexに対応)の重要度をはかるという方法に基づいたWSDに関するお話．</p>

<p>具体的には、まずコンテキスト（本論文では文）内の単語の各語義をグラフのノードとして初期化し、それぞれの語義からグラフ内のほかの語義へのリンクをDFSで探し，みつかればそのルート中の語義をグラフに追加していき，その間にエッジを張る．ということを行っている．</p>

<p>この操作で出来たグラフの「あるノード」の重要度が高いようであれば，その語義はコンテキスト内での重要度が高く，語義候補である可能性が高い，という直感に基づいている．</p>

<p>肝心の重要度の尺度であるが，彼らは local な尺度と globalな尺度の大きく分けて二種類の尺度について実験を行っている．</p>

<p>localな尺度を用いた手法では、グラフ内のノードを尺度に基づいてランキングし、もっともSenses(wi)に対応するノードの中で最も高いランクを得た語義を出力する仕組み．</p>

<p>globalな尺度はグラフ全体に対してスカラーの値を与えるものなので，語義選択に対してはそのままでは用いることができない．そこで， G の部分集合となるような G&#8217;(コンテキスト内の単語それぞれについて語義ひとつだけを考えたもの) を考え，そのサブグラフのglobal measureの値が高いものを語義の組み合わせとして導きだす． たとえば，文が 2単語から成っており、それぞれが， 3つ，4つの語義を持っている場合は、3 *4 で12個のサブグラフを作り，それぞれに対して global measureを計算する。もっとも高いglobal measueを得たサブグラフを語義の組み合わせとして出力する．（計算量が爆発するという意味でオリジナルのLeskアルゴリズムと類似している）</p>

<p>過去の研究として，</p>

<ul>
<li>R. Barzilay and M. Elhadad, “Using Lexical Chains for Text Summarization,” Proc. ACL Workshop Intelligent Scalable Text Summarization, pp. 10-17, 1997.</li>
<li>R. Mihalcea, “Unsupervised Large-Vocabulary Word Sense Disambiguation with Graph-Based Algorithms for Sequence Data Labeling,” Proc. Human Language Technology and Empirical Methods in Natural Language Processing, pp. 411-418, 2005</li>
<li>M. Galley and K. McKeown, “Improving Word Sense Disambiguation in Lexical Chaining,” Proc. 18th Int’l Joint Conf. Artificial Intelligence, pp. 1486-1488, 2003.</li>
</ul>


<p>が比較に挙げられているが，本論文の手法は one sense per one discourse を強く仮定しない(同じドキュメント内で同じ対象語に対してもコンテキストが異なれば違う語義が出力されうる)．また、グラフをunlabeled &amp; unweightedに構築している点が異なる．これには以下の二つの理由があるらしい．</p>

<ul>
<li>広く合意を得たweightingの方法が確立されていない</li>
<li>研究の焦点を絞りたい(まぁweightingは今のところ特に興味ない)</li>
</ul>


<p>また、WordNet にくっついている MFS(というより，語義頻度) は本論文ではつかっていない．なんでかというと，それはhand-labeledなSemcorコーパスから得られたものであり，他の言語とかドメインに対しても有効な指標とは言えないから（この点は激しく同意）．</p>

<p>以下，それぞれの尺度についてまとめる．</p>

<h3>local measure</h3>

<p>local measureは特定のグラフのノードの重要度を表すもの．あるノードのグラフ全体に対する影響度，とみなすこともできる 値域は [0,1] で、1に近ければ重要、0に近ければ重要ではない．</p>

<ul>
<li><p><strong>Degree</strong> これは単なる次数。deg(v) = vの次数とすると， C_degree(v) = deg(v) / (|V| - 1)というように正規化しておく</p></li>
<li><p><strong>Eigenvector</strong> ようは PagerankとHITS．特筆すべきことはなし．</p></li>
<li><p><strong>KPP</strong>（あとで読む)</p></li>
<li><p><strong>Betweeness</strong> shortest pathの数を用いる． σ<em>st = s->tへのshortest pathの数，　σ</em>st(v) そのうち、vを通るもの．σ<em>st(v) / σ</em>st をすべてのノードペアに対して和をとって，betweeness(v)とする．そして(|V| - 1)(|V| - 2)で割って正規化．</p></li>
</ul>


<h3>global measure</h3>

<p>さきほどのlocal measureがグラフのノードに対して重要度を与えるものだったのに対して，こちらは特定の語義の組み合わせからなるグラフに対して[0,1]のスカラー値を与えるもの．</p>

<ul>
<li><strong>Compactness, Graph entropy, Edge density</strong> いわゆるグラフのコンパクト性などの一般的な尺度</li>
</ul>


<p>global measureは計算量的に無理がある（文内のすべての単語に対する語義の組み合わせを列挙してそれぞれに対して求めなければならない)ので工夫をしている．</p>

<h4>global measure計算における工夫</h4>

<ul>
<li><p><strong>Simulated Annealing</strong> まずランダムに語義選択を初期化，ひとつ取りかえて上記 global measure の差分(ΔE)をみる．もしよくなっていれば採択、悪くなっている場合でも exp(ΔE/T)の確率で採択．Tは何らかの定数，これを u 回繰り返した結果を採用．書いてて思いましたが，Simulated Annealingというよりは，Metropolis Hastingsですねこれ．</p></li>
<li><p><strong>Genetic Algorithms</strong>
なんか面倒そうなので略．結局パラメータ調整は面倒だしあんまりいいこと無い，みたいな結論に至っており，若干残念な感じ．</p></li>
</ul>


<h3>Complexity</h3>

<p>まず初期グラフのノード数について，</p>

<p>k = WordNet内の語の最大の語彙数(最も多い語義を持つ語の語義数)</p>

<p>n = sentence σの長さ
|V_σ| は knのオーダーだけど、もう少しタイトに上界を抑えることはできるようだ</p>

<p>グラフ作成には、だいたい O(n<sup>2</sup>
)くらい。
各尺度の計算量については，論文中のTable3に書いてある．</p>

<h3>Experiment</h3>

<p>データはSemCor, Senseval-3, Semeval-2007．Sensemapつかって全部WordNet2.0にマップしてある．</p>

<p>Sense-inventoryにはWordNet2.0 と EnWordNetというものを使っている．EnWordNetはcollocational relationをもちいてedgeを6万本くらい増やしたWordNetらしい．具体的な構築方法はよく分からないが． WN++みたいなものか．</p>

<p>ふつうのWordNetから構築したグラフと，EnWordNetから構築したグラフの性質の比較も行っている．</p>

<ul>
<li><p><strong>small world effect</strong> : l = グラフのスモールワールド性（任意の頂点ペアの間の最短距離の平均)</p></li>
<li><p><strong>clustering rate(or transitivity)</strong> : C = probability that two neighbors of a vertex are connected 詳しくは式参照 ネットワーク内の三角形の数で計算するらしい。</p></li>
<li><p><strong>cumulative degree distribution</strong> : P_k = Σ p_k&#8217; (&lt;- k次のノードの割合)</p></li>
</ul>


<p>結果として，EnWordNetから構築したグラフのほうがdenseでござるという当たり前の議論が．(そりゃあエッジ足してるだけなのだから当然)</p>

<p>グラフ生成におけるDFSの深さは6にしている（SemCor上で実験して決めたらしい)</p>

<p>その他、GA, SAのパラメータについてうんたらかんたら．</p>

<h4>Result</h4>

<p>Degree（もっとも単純なlocal measure,ノードの次数そのもの) がもっともよく，いろいろ工夫した他のlocal measureやglobal measureは振るわなかった．残念．．．</p>

<p>WordNet vs EnWordNetの比較では，EnWordNetの方が若干ながら良い結果．</p>

<p>しかしながら，unsupervised WSDの常として，MFS(First Sense Baseline)は非常に強く，F1 measureで20ポイント以上の差をつけられてしまっている．</p>

<h3>感想</h3>

<p>グラフとしては最も基本的な， unlabeled, undirected, unweightedなものの上で何ができるか，を追求した研究といえる．</p>

<p>単純な指標がうまくいってしまい，いろいろ工夫しても無駄っぽいのは残念だが，unsupervised WSDの難しさは語義粒度とknowledge baseの質如何でいくらでも変動しうるので，言語資源の整備が進めば，また違った結果が得られるかもしれない．</p>

<p>WSDにおいて宿命になっている，&#8221;MFSに勝てない問題&#8221;については，近年は</p>

<ul>
<li><a href="http://www.aclweb.org/anthology/P/P10/P10-1154.pdf">SP Ponzetto, R Navigli &#8220;Knowledge-rich Word Sense Disambiguation Rivaling Supervised Systems&#8221; Proc of ACL 2010</a></li>
</ul>


<p>のように，知識ベース(WordNet)側をガンガン強化することで，ほぼ遜色ない性能を出すことができる手法も出てきている．しかし，こちらも，もっとも性能の良い尺度はDegreeなので，なんだかなぁという気はする．</p>

<p>結局知識ベースの品質に強く依存するknowledge based WSDではあるが，まだ何かできることは無いかと考えると，うまくいくかはともかくとして，いろいろ面白そうだ．（ニッチなためか，機械学習屋さんがまだあまり進出してこない分野でもあるので）</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Posterior Regularization と Unified Expectation Maximizationについて]]></title>
    <link href="http://conditional.github.com/blog/2013/03/23/about-posterior-regularization-and-unified-expectation-maximization/"/>
    <updated>2013-03-23T16:20:00+09:00</updated>
    <id>http://conditional.github.com/blog/2013/03/23/about-posterior-regularization-and-unified-expectation-maximization</id>
    <content type="html"><![CDATA[<p>桜がとってもきれいですね．すずかけ台は8分咲きといったところです．ところで，仲間で行っている小規模な勉強会で</p>

<ul>
<li><a href="http://www.aclweb.org/anthology/N/N12/N12-1087.pdf">&#8220;Unified Expectation Maximization&#8221;</a> Samdani et al, NAACL2012</li>
</ul>


<p>を紹介してきたので，資料をslideshareにあげておきました．</p>

<iframe src="http://www.slideshare.net/slideshow/embed_code/17552885" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen webkitallowfullscreen mozallowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="http://www.slideshare.net/koji_matsuda/unified-expectation-maximization" title="Unified Expectation Maximization" target="_blank">Unified Expectation Maximization</a> </strong> from <strong><a href="http://www.slideshare.net/koji_matsuda" target="_blank">koji_matsuda</a></strong> </div></p>

<p>Unified EMというと，じゃっかん大風呂敷な感じのタイトルですが，キーとなるアイデアはとても単純で，EMアルゴリズムのE-Stepで最小化するKLダイバージェンスにちょっと細工を入れることで，Hard-EMとふつうのEMの中間くらいの性質を持ったアルゴリズムになりますよ．というお話です．Deterministic Annealing EMの逆バージョンみたいな雰囲気(実際，DAEMもこの枠組で書けることが示されています) 手元にEMのコードがあれば，実装も非常に容易．</p>

<p>ただ，やっぱりそれだけだと一発ネタにしかならないので，「制約付きEM」のほうへ話を進めています．「制約付きEM」というと聞きなれないアルゴリズムですが，Un(semi-)supervised learningにおいて，事前知識を用いてモデルがとんでもない方向へ飛ぶのを防ごう，というモチベーションに基づく技法のようです．</p>

<p>自然言語処理におけるアプリケーションでは，たとえば以下のような制約を考えることができます：</p>

<ul>
<li><p>品詞タグ付けなら</p>

<ul>
<li>ある文には，名詞と動詞が最低一つづつ含まれる</li>
<li>ある語が，複数のPOSに割り当てられることは稀</li>
</ul>
</li>
<li><p>機械翻訳におけるアラインメントなら</p>

<ul>
<li>L1->L2のアラインメントと，L2->L1のアラインメントは一致する</li>
<li>L1の一つの語が，L2の多数の語と対応付けられることは稀</li>
</ul>
</li>
<li><p>関係抽出なら</p>

<ul>
<li>ある種のエンティティと，ある種のエンティティの間には，特定のリレーションしか成り立たない

<ul>
<li>(PERSON, LOCATION) -> LIVE IN</li>
<li>(ORGANIZATION, PERSON) -> WORK FOR</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>こういった事前知識に基づく制約を満たすモデルを，確率分布の集合として表現し，そこから離れないようにEMアルゴリズムを行うことによって，ラベルつきデータが利用可能ではない（または，少量しか存在しない）状況において，うまく学習が行おうというのが，「制約付きEM」の肝となる部分です．直感的には，Posterior Regularizationの論文から引用した以下の図が分かりやすいかもしれません．（日本語注釈は私によるものです）</p>

<p><img src="http://conditional.github.com/images/posterior_regularization_fig.png" width="460" height="302"></p>

<p>歯切れのよいタイトルに惹かれて軽い気持ちで選んだ論文でしたが，そこそこホットな分野のようで，ACL 2011のチュートリアルで1トラックまるまるこの話題だったりしたらしく，問題設定や前提を理解するのにけっこう苦労しました．</p>

<p>結果として，UEMの本題ではなく，問題設定や先行研究の紹介に半分近くのスライドを割くことに・・・．まぁ，楽しんで頂けたようなのでなによりです．しかしひさびさに緊張感のあるプレゼンだった．</p>

<p>最後のスライドにも記載しましたが，以下の文献が，理解の助けになると思います．</p>

<ul>
<li><a href="http://sideinfo.wikkii.com/">&#8220;Rich Prior Knowledge in Learning for Natural Language Processing&#8221;</a> ACL 2011 tutorial

<ul>
<li>ACLで行われたチュートリアルの資料です．このファミリーに属するアルゴリズムについての資料の多くはここからたどれるようになっています．各種のアルゴリズムのあいだの関係についてよくまとまっていますし，著者らによって異なる表記系もすっきりまとめられているので読みやすい．今回は取り上げませんでしたが，Labeled Feature とかも追ってみるとおもしろそうなトピックなので，ぜひ．</li>
</ul>
</li>
<li><a href="http://jmlr.csail.mit.edu/papers/volume11/ganchev10a/ganchev10a.pdf">&#8220;Posterior Regularization for Structured Latent Variable Models&#8221;</a> JMLR 2010

<ul>
<li>今回紹介した論文の元ネタになっている Posterior Regularizationの論文です．品調ラベルづけにおける例とともに，少しづつ丁寧に議論が進められており，さすがジャーナルだけあって読みやすいです．</li>
</ul>
</li>
</ul>


<p>ひさびさに負荷の高い一週間だったので，来週はすこしゆっくりしたいと思います．（日記）</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Prowl+zshで快適お昼寝タイム]]></title>
    <link href="http://conditional.github.com/blog/2013/03/14/take-a-nap-with-prowl/"/>
    <updated>2013-03-14T17:59:00+09:00</updated>
    <id>http://conditional.github.com/blog/2013/03/14/take-a-nap-with-prowl</id>
    <content type="html"><![CDATA[<p>眠いですね．</p>

<p>とくに機械学習のクロスバリデーションや，ごっつい集計クエリなどの時間のかかるバッチジョブを流す間，とても眠い．</p>

<p>私の場合，そういう時にはディスプレイの前を離れて，お昼寝タイムにすることが多いです（基本いつも眠い）．実行時間の見積もりがつくようなジョブなら適当にアラームかけておけばよいのですが，実際はそうも行かないことも多いですよね．</p>

<p>そこで，iOSデバイスにプッシュ通知を送れるアプリケーション <strong>Prowl</strong> を用いて，ジョブの終了をiPhoneに通知してくれる短いRubyスクリプトを書いてみました．ジョブ終わったらブルッと鳴って目覚めスッキリ．</p>

<h2>Requirement</h2>

<ul>
<li><a href="https://www.prowlapp.com/">Prowl</a></li>
<li><a href="https://github.com/augustl/ruby-prowl">prowl gem</a></li>
</ul>


<p>gem は <code>gem intall prowl</code> でインストールできます．</p>

<h2>Instalation</h2>

<p>まず，ProwlのウェブサイトからAPIキーを取得します．いちおう登録＆ログインが必要ですが，メールアドレスは不要のようです，ログインして，<strong>API keys</strong> のページに進むと
フォームが二つありますが，Provider keyは第三者にキー入りアプリを配布するときに使うものなので，今回は <strong>Generate a new API key</strong> のほうでOK．Noteは空でもいいのでとにかくサブミットボタンを押すと，API keyが取得できます．</p>

<p><img src="http://conditional.github.com/images/generate_prowl_apikey.png"></p>

<p>そんでもってスクリプトの中身はこんな感じ．<code>API_KEY</code>には先ほど取得したキーを．</p>

<div><script src='https://gist.github.com/5161032.js'></script>
<noscript><pre><code></code></pre></noscript></div>


<p>環境変数 <code>PROWL_NOTICE_TITLE</code>, <code>PROWL_NOTICE_CONT</code> にメッセージをセットしておくと，その中身を通知してくれます．通知を受け取りたいプロセスと環境変数の設定をラップするようなシェルスクリプトを組んでおくと良いかもしれません．<code>ARGV</code> についてはあとで説明します．</p>

<h2>Example</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sleep 10 && ruby prowlnotification.rb</span></code></pre></td></tr></table></div></figure>


<p>という感じでプロセスを実行すると，終了時にこんな感じで通知してくれます．</p>

<p><img src="http://conditional.github.com/images/prowl_sleep.jpg" width="320" height="480"></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ export PROWL_NOTICE_TITLE='たいとるだよ'
</span><span class='line'>$ export PROWL_NOTICE_DESC='おわったよ〜'
</span><span class='line'>$ ruby prowlnotification.rb</span></code></pre></td></tr></table></div></figure>


<p>とかするとこんな感じ．</p>

<p><img src="http://conditional.github.com/images/prowl_with_env.jpg" width="320" height="480"></p>

<h2>zsh(precmd/preexec)との組み合わせ</h2>

<p>しかし，毎回コマンド打つのは面倒ですね．そこで，<a href="http://umezo.hatenablog.jp/entry/20100508/1273332857">処理時間が一定以上かかったらGrowlで通知するzshrc - 心魅 - cocoromi</a> で紹介されている方法を使うと，
コマンドに一定以上の時間がかかったときに自動で通知してくれます．
今回のスクリプトに合わせてちょっと改造してみました．<code>PROWL_NOTICE_TIME</code>を適当に設定して<code>.zshrc</code>に潜ませてみてください．</p>

<div><script src='https://gist.github.com/5161045.js'></script>
<noscript><pre><code></code></pre></noscript></div>


<p>とくに環境変数が設定されていない場合は，プロセスの名前と引数が通知されてきます．(<code>ARGV</code>を通してRubyスクリプトに渡されます)</p>

<h2>まとめ</h2>

<ul>
<li>眠い時でも思う存分バッチを走らせることができるRubyスクリプトを書いてみました．</li>
<li>実行に一定時間以上かかったら自動で通知してくれるzshの設定例を紹介しました．</li>
<li>勤務時間中の居眠りは計画的に．</li>
</ul>


<p>Prowlは有料(執筆現在250円)アプリですが，単に通知を受け取るだけではなく，通知を他のアプリにフォワードしてくれる機能があったり，<a href="http://www.prowlapp.com/apps.php">Chromeで開いているページを通知してくれるプラグイン</a>が用意されていたり，そこそこ遊べそうなので，今回導入してみました．</p>

<p>もし同様の機能の無料アプリをご存知でしたら是非教えてください．</p>

<h2>参考</h2>

<ul>
<li><a href="http://umezo.hatenablog.jp/entry/20100508/1273332857">処理時間が一定以上かかったらGrowlで通知するzshrc - 心魅 - cocoromi</a></li>
<li><a href="https://www.prowlapp.com/">Prowl</a></li>
<li><a href="https://github.com/augustl/ruby-prowl">prowl gem</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Start Bloging With Octopress]]></title>
    <link href="http://conditional.github.com/blog/2013/03/13/start-bloging-with-octopress/"/>
    <updated>2013-03-13T16:51:00+09:00</updated>
    <id>http://conditional.github.com/blog/2013/03/13/start-bloging-with-octopress</id>
    <content type="html"><![CDATA[<p>とりあえずはじめてみた．</p>

<p><a href="http://www.miukoba.net/blog/2013/01/05/start-octopress/">Octopressはじめました - mimemo</a></p>

<p>が大変参考になりました．<a href="http://mouapp.com/">Mou</a>便利．markdown，必要に迫られたときしか
書いてこなかったんだけど，少しづつ慣れていきたい．</p>

<p>数式のテスト(<a href="https://gist.github.com/jessykate/834610">MathJax.rb</a>利用)</p>

<script type="math/tex; mode=display">
N(m,\sigma^{2})=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-m)^2}{2\sigma^{2}}}
</script>

]]></content>
  </entry>
  
</feed>
